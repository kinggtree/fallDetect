{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eaad61",
   "metadata": {},
   "source": [
    "# Notice\n",
    "训练之前请确保，特征模型训练后得到的 **`feature_model_1dcnn.pth`** 和 **`scaler_50hz_torch.gz`** 这两个文件存在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7145f7",
   "metadata": {},
   "source": [
    "# 导入依赖库，定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a246b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "SEQUENCE_LENGTH = int(TARGET_SAMPLING_RATE_HZ * 4) # 200 samples for 4 seconds at 50Hz\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * 1)          # 50 samples for 1 second step at 50Hz\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91559b",
   "metadata": {},
   "source": [
    "# 第 1 步：处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcd81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for data in: MobiFall_Dataset\n",
      "\n",
      "Processing and combining 627 unique trials...\n",
      "Successfully processed and combined sensor data for 627 trials.\n"
     ]
    }
   ],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < SEQUENCE_LENGTH: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "# def create_sequences(data_list, label_list, seq_length, step):\n",
    "#     \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "#     # 初始化用于存放最终序列和对应标签的列表\n",
    "#     X, y = [], []\n",
    "#     # 遍历每一次活动试验的数据\n",
    "#     for i, trial_data in enumerate(data_list):\n",
    "#         trial_label = label_list[i]\n",
    "#         # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "#         for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "#             # 截取一个固定长度（seq_length）的片段作为序列\n",
    "#             X.append(trial_data[j:(j + seq_length)])\n",
    "#             # 为这个序列分配对应的标签\n",
    "#             y.append(trial_label)\n",
    "            \n",
    "#     if not X: return np.array([]), np.array([])\n",
    "#     # 将列表转换为Numpy数组后返回\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "# x, y = create_sequences(trial_arrays, trial_labels, SEQUENCE_LENGTH, STEP)\n",
    "# print(f\"The shape of X: {x.shape}, The shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e947c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous data shape: (593473, 11)\n"
     ]
    }
   ],
   "source": [
    "def create_continuous_stream(data_list: list, label_list: list) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    将所有试验数据拼接成一个连续的、无采样的原始数据流，并生成对应的标签流。\n",
    "\n",
    "    参数:\n",
    "    - data_list (list): 一个列表，其中每个元素是一个试验的Numpy数组 (形状为 [n_samples, 11])。\n",
    "    - label_list (list): 一个列表，包含每个试验对应的标签 (0 或 1)。\n",
    "\n",
    "    返回:\n",
    "    - continuous_data (np.ndarray): 拼接后的连续数据流，形状为 [总时间点数, 11]。\n",
    "    - continuous_labels (np.ndarray): 对应的连续标签流，形状为 [总时间点数,]。\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # 为每个试验的每个时间点生成对应的标签\n",
    "    # 例如，如果一个试验有491个时间点，标签是1，我们就生成一个包含491个1的数组\n",
    "    all_trial_labels_expanded = []\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        num_timesteps = trial_data.shape[0]  # 获取该试验的时间点数量\n",
    "        trial_label = label_list[i]\n",
    "        # 使用 np.full 创建一个长度为 num_timesteps，值全为 trial_label 的数组\n",
    "        labels_for_this_trial = np.full(num_timesteps, trial_label, dtype=np.int32)\n",
    "        all_trial_labels_expanded.append(labels_for_this_trial)\n",
    "\n",
    "    # 使用 np.concatenate 将所有试验数据数组沿第一个轴（时间轴）拼接起来\n",
    "    continuous_data = np.concatenate(data_list, axis=0)\n",
    "    \n",
    "    # 同样地，拼接所有扩展后的标签数组\n",
    "    continuous_labels = np.concatenate(all_trial_labels_expanded, axis=0)\n",
    "\n",
    "    return continuous_data, continuous_labels\n",
    "\n",
    "continuous_data, continuous_labels = create_continuous_stream(trial_arrays, trial_labels)\n",
    "print(f\"Continuous data shape: {continuous_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6fe3",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a0d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1):\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 200 -> 100\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 100 -> 50\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: 50 -> 25\n",
    "        )\n",
    "        \n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        # 输入维度需要计算: 256 (channels) * 25 (length)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 25, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        标准的前向传播，用于训练和评估\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # Conv1d 需要 (N, C, L) 格式, 所以我们需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        # 因为使用 BCEWithLogitsLoss, 所以不需要在这里加 sigmoid\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        仅用于提取中间特征的函数\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # 同样需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        # 只通过特征提取器\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 输出形状将是 (N, 256, 25)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c20b2f",
   "metadata": {},
   "source": [
    "# 第 2 步：生成连续的特征流和标签流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型和标准化器...\n",
      "模型已从 feature_model_1dcnn.pth 加载\n",
      "标准化器已从 scaler_50hz_torch.gz 加载\n",
      "\n",
      "开始批量提取特征...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 3. 从模型中提取特征\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# 关闭梯度计算以加速\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 4. 将特征扁平化并添加到列表中\u001b[39;00m\n\u001b[0;32m     59\u001b[0m flattened_features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[1;32mIn[4], line 59\u001b[0m, in \u001b[0;36mFeatureModel1DCNN.extract_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# -> (N, 11, 200)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 只通过特征提取器\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 输出形状将是 (N, 256, 25)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:135\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\nn\\functional.py:1701\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1699\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1701\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "# 确保这些参数与模型训练和模拟逻辑一致\n",
    "WINDOW_SECONDS = 4\n",
    "\n",
    "WINDOW_SIZE = int(TARGET_SAMPLING_RATE_HZ * WINDOW_SECONDS) # 窗口大小 (200个点)\n",
    "STEP_SIZE = 25 # 步长 (25个点)\n",
    "\n",
    "MODEL_PATH = \"feature_model_1dcnn.pth\"\n",
    "SCALER_PATH = \"scaler_50hz_torch.gz\"\n",
    "\n",
    "# --- 2. 加载模型和标准化器 ---\n",
    "print(\"正在加载模型和标准化器...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载模型\n",
    "model = FeatureModel1DCNN(input_channels=11, num_classes=1).to(device)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f\"模型已从 {MODEL_PATH} 加载\")\n",
    "else:\n",
    "    print(f\"警告: 在 {MODEL_PATH} 未找到模型文件。将使用随机初始化的模型。\")\n",
    "model.eval() # 设置为评估模式\n",
    "\n",
    "# 加载标准化器\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(f\"标准化器已从 {SCALER_PATH} 加载\")\n",
    "else:\n",
    "    # 抛出错误并停止执行\n",
    "    raise FileNotFoundError(f\"ERROR: Standard scaler file not found at '{SCALER_PATH}'. Cannot proceed without it.\")\n",
    "\n",
    "\n",
    "# --- 3. 批量处理数据并提取特征 ---\n",
    "print(\"\\n开始批量提取特征...\")\n",
    "all_features_list = []\n",
    "all_labels_list = []\n",
    "\n",
    "# `trial_arrays` 和 `trial_labels` 变量是从上一个数据加载单元格中获得的\n",
    "# 遍历每一次试验的数据\n",
    "for i in range(len(continuous_data) - WINDOW_SIZE + 1):\n",
    "    trial_label = continuous_labels[i]\n",
    "\n",
    "    window = continuous_data[i : i + WINDOW_SIZE]\n",
    "    \n",
    "    # 2. 预处理窗口数据 (标准化 -> 转换为Tensor)\n",
    "    scaled_window = scaler.transform(window)\n",
    "    window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # 3. 从模型中提取特征\n",
    "    with torch.no_grad(): # 关闭梯度计算以加速\n",
    "        features = model.extract_features(window_tensor)\n",
    "    \n",
    "    # 4. 将特征扁平化并添加到列表中\n",
    "    flattened_features = features.cpu().numpy().flatten()\n",
    "    all_features_list.append(flattened_features)\n",
    "    \n",
    "    # 5. 将该窗口对应的标签添加到列表中\n",
    "    all_labels_list.append(trial_label)\n",
    "\n",
    "print(f\"处理完成！共处理了 {len(trial_arrays)} 次试验，生成了 {len(all_features_list)} 个特征向量。\")\n",
    "\n",
    "# --- 4. 保存最终的数据集 ---\n",
    "if all_features_list:\n",
    "    # 将列表转换为Numpy数组\n",
    "    final_features = np.array(all_features_list)\n",
    "    final_labels = np.array(all_labels_list)\n",
    "\n",
    "    # 保存数组到.npy文件\n",
    "    np.save(\"all_features.npy\", final_features)\n",
    "    np.save(\"all_labels.npy\", final_labels)\n",
    "\n",
    "    print(f\"\\n数据集已成功保存:\")\n",
    "    print(f\"  - 特征文件: all_features.npy, 形状: {final_features.shape}\")\n",
    "    print(f\"  - 标签文件: all_labels.npy, 形状: {final_labels.shape}\")\n",
    "else:\n",
    "    print(\"\\n未能生成任何特征，未创建文件。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f7f19",
   "metadata": {},
   "source": [
    "# 第 3 步：生成场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a680bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading foundational data streams...\n",
      "--- Starting Scenario Generator ---\n",
      "Step 1: Aligning Low-Fidelity and High-Fidelity streams...\n",
      "Streams aligned. Resulting length: 593274 points.\n",
      "Step 2: Generating scenarios with Window-Level Sparsity...\n",
      "Generated 593172 scenarios.\n",
      "Example: 177811 scenarios have raw data available.\n",
      "\n",
      "Splitting data into training and validation sets...\n",
      "FidelityDataset created with 474537 samples.\n",
      "FidelityDataset created with 118635 samples.\n",
      "\n",
      "--- DataLoader Sanity Check ---\n",
      "Feature sequence batch shape: torch.Size([64, 60, 6400])\n",
      "Raw data batch shape: torch.Size([64, 200, 11])\n",
      "Labels batch shape: torch.Size([64])\n",
      "\n",
      "Dataset construction complete. The `train_loader` and `val_loader` are ready for training.\n"
     ]
    }
   ],
   "source": [
    "def create_training_scenarios(all_features, continuous_raw_data, continuous_labels,\n",
    "                              window_size, history_len, availability_ratio):\n",
    "    \"\"\"\n",
    "    基于连续的LFS和HFS，生成用于训练保真模型的多样化场景。\n",
    "\n",
    "    :param all_features: 低保真特征流 (LFS), (N, feature_dim)\n",
    "    :param continuous_raw_data: 连续的原始数据流 (HFS), (M, 11)\n",
    "    :param continuous_labels: 连续的标签流, (M,)\n",
    "    :param window_size: 生成单个特征所需的原始数据窗口大小 (200)\n",
    "    :param history_len: 保真模型LSTM所需的历史特征序列长度\n",
    "    :param availability_ratio: 原始数据在任一时间点可用的概率\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Scenario Generator ---\")\n",
    "    \n",
    "    # 1. 对齐数据流\n",
    "    print(\"Step 1: Aligning Low-Fidelity and High-Fidelity streams...\")\n",
    "    offset = window_size - 1\n",
    "    aligned_hfs = continuous_raw_data[offset:]\n",
    "    aligned_labels = continuous_labels[offset:]\n",
    "    \n",
    "    # 确保LFS和对齐后的HFS/Labels长度一致\n",
    "    num_features = len(all_features)\n",
    "    aligned_hfs = aligned_hfs[:num_features]\n",
    "    aligned_labels = aligned_labels[:num_features]\n",
    "    \n",
    "    print(f\"Streams aligned. Resulting length: {num_features} points.\")\n",
    "    \n",
    "    # 2. 生成场景\n",
    "    print(\"Step 2: Generating scenarios with Window-Level Sparsity...\")\n",
    "    scenarios = []\n",
    "    \n",
    "    # 从第一个可以构成完整历史序列的点开始遍历\n",
    "    for t in range(history_len - 1, num_features):\n",
    "        # 提取历史特征序列\n",
    "        feature_sequence = all_features[t - history_len + 1 : t + 1]\n",
    "        \n",
    "        # 模拟采样决策\n",
    "        raw_data_for_t = None\n",
    "        if random.random() < availability_ratio:\n",
    "            # 如果“采样”成功，则获取当前时间点对应的原始数据窗口\n",
    "            start_index = t - (window_size - 1)\n",
    "            end_index = t + 1\n",
    "            raw_data_for_t = aligned_hfs[start_index : end_index]\n",
    "            \n",
    "            # 安全检查，确保切片长度正确\n",
    "            if raw_data_for_t.shape[0] != window_size:\n",
    "                continue # 如果窗口不完整（通常在数据流末尾），则跳过此样本\n",
    "        \n",
    "        # 提取标签\n",
    "        label = aligned_labels[t]\n",
    "\n",
    "        # 存入场景列表\n",
    "        scenarios.append((feature_sequence, raw_data_for_t, label))\n",
    "        \n",
    "    print(f\"Generated {len(scenarios)} scenarios.\")\n",
    "    print(f\"Example: {len([s for s in scenarios if s[1] is not None])} scenarios have raw data available.\")\n",
    "    return scenarios\n",
    "\n",
    "\n",
    "# PyTorch Dataset\n",
    "class FidelityDataset(Dataset):\n",
    "    def __init__(self, scenarios):\n",
    "        self.scenarios = scenarios\n",
    "        print(f\"FidelityDataset created with {len(self.scenarios)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scenarios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_seq, raw_data, label = self.scenarios[idx]\n",
    "\n",
    "        feature_seq_tensor = torch.tensor(feature_seq, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        if raw_data is not None:\n",
    "            raw_data_tensor = torch.tensor(raw_data, dtype=torch.float32)\n",
    "        else:\n",
    "            # 当原始数据不存在时，用一个全零的Tensor作为占位符\n",
    "            # 这就是对“完全缺失”的窗口级“全零”填补\n",
    "            raw_data_tensor = torch.zeros((200, 11), dtype=torch.float32)\n",
    "            \n",
    "        return feature_seq_tensor, raw_data_tensor, label_tensor\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. 配置参数 ---\n",
    "HISTORY_LEN = 60      # 保真模型的历史窗口长度 (30秒)\n",
    "WINDOW_SIZE = 200     # 特征模型用的原始数据窗口长度 (4秒)\n",
    "AVAILABILITY_RATIO = 0.3 # 原始数据可用率 (稀疏度)\n",
    "\n",
    "# --- 2. 加载基础数据流 ---\n",
    "print(\"Loading foundational data streams...\")\n",
    "\n",
    "try:\n",
    "    all_features = np.load(\"all_features.npy\")\n",
    "    all_labels = np.load(\"all_labels.npy\") # 这里的标签是对应每个特征的\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: `all_features.npy` or `all_labels.npy` not found.\")\n",
    "    print(\"Please generate these files first or use the dummy data block.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. 调用场景生成器 ---\n",
    "all_scenarios = create_training_scenarios(\n",
    "    all_features=all_features,\n",
    "    continuous_raw_data=continuous_data,\n",
    "    continuous_labels=continuous_labels,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    history_len=HISTORY_LEN,\n",
    "    availability_ratio=AVAILABILITY_RATIO\n",
    ")\n",
    "\n",
    "# --- 4. 准备训练、验证集 ---\n",
    "print(\"\\nSplitting data into training and validation sets...\")\n",
    "\n",
    "# 从场景中解包，以便进行分层抽样\n",
    "feature_sequences, raw_data_list, labels = zip(*all_scenarios)\n",
    "\n",
    "# 划分训练集和验证集 (80/20)\n",
    "X_seq_train, X_seq_val, X_raw_train, X_raw_val, y_train, y_val = train_test_split(\n",
    "    feature_sequences, raw_data_list, labels, \n",
    "    test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# 重新打包成场景元组\n",
    "train_scenarios = list(zip(X_seq_train, X_raw_train, y_train))\n",
    "val_scenarios = list(zip(X_seq_val, X_raw_val, y_val))\n",
    "\n",
    "# 创建Dataset和DataLoader\n",
    "train_dataset = FidelityDataset(train_scenarios)\n",
    "val_dataset = FidelityDataset(val_scenarios)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n--- DataLoader Sanity Check ---\")\n",
    "# 从dataloader中取出一个批次的数据来检查\n",
    "batch_feature_seq, batch_raw_data, batch_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Feature sequence batch shape: {batch_feature_seq.shape}\")\n",
    "print(f\"Raw data batch shape: {batch_raw_data.shape}\")\n",
    "print(f\"Labels batch shape: {batch_labels.shape}\")\n",
    "\n",
    "print(\"\\nDataset construction complete. The `train_loader` and `val_loader` are ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fa41c",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062ba0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_data_cnn():\n",
    "    \"\"\"创建一个用于处理原始传感器数据的1D-CNN模块。\"\"\"\n",
    "    raw_data_processor = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=11, out_channels=64, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(64),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    return raw_data_processor\n",
    "\n",
    "class FidelityModelGated(nn.Module):\n",
    "    def __init__(self, feature_dim, lstm_hidden_dim, raw_cnn_output_dim, num_classes=1):\n",
    "        super(FidelityModelGated, self).__init__()\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.raw_cnn_output_dim = raw_cnn_output_dim\n",
    "        self.feature_lstm = nn.LSTM(\n",
    "            input_size=feature_dim, hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2, batch_first=True, dropout=0.5\n",
    "        )\n",
    "        self.raw_data_cnn = create_raw_data_cnn()\n",
    "        self.gating_layer = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim + self.raw_cnn_output_dim, lstm_hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(lstm_hidden_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "        self.raw_transform = nn.Linear(self.raw_cnn_output_dim, self.lstm_hidden_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.5), nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, raw_data=None):\n",
    "        lstm_outputs, (h_n, c_n) = self.feature_lstm(feature_sequence)\n",
    "        lstm_last_output = lstm_outputs[:, -1, :]\n",
    "        if raw_data is not None and raw_data.nelement() > 0: # 确保传入的不是空tensor\n",
    "            raw_data = raw_data.permute(0, 2, 1)\n",
    "            v_raw = self.raw_data_cnn(raw_data)\n",
    "        else:\n",
    "            v_raw = torch.zeros(feature_sequence.size(0), self.raw_cnn_output_dim, device=feature_sequence.device)\n",
    "\n",
    "        combined_for_gate = torch.cat((lstm_last_output, v_raw), dim=1)\n",
    "        gate = self.gating_layer(combined_for_gate)\n",
    "        transformed_v_raw = self.raw_transform(v_raw)\n",
    "        fused_vector = lstm_last_output + gate * torch.tanh(transformed_v_raw)\n",
    "        logits = self.classifier(fused_vector)\n",
    "        state_feature = h_n[-1, :, :].squeeze(0)\n",
    "        return logits, state_feature, gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99ba2",
   "metadata": {},
   "source": [
    "# 第 4 步：训练与评估函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, noise_level=0.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for feature_seq, raw_data, labels in dataloader:\n",
    "        feature_seq = feature_seq.to(device)\n",
    "        raw_data = raw_data.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1) # 形状匹配BCEWithLogitsLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 注意：只关心训练时的logits输出\n",
    "        logits, _, _ = model(feature_seq, raw_data)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * feature_seq.size(0)\n",
    "    \n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for feature_seq, raw_data, labels in dataloader:\n",
    "            feature_seq = feature_seq.to(device)\n",
    "            raw_data = raw_data.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            logits, _, _ = model(feature_seq, raw_data)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * feature_seq.size(0)\n",
    "            \n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032eaed",
   "metadata": {},
   "source": [
    "# 第 6 步：训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model for training...\n",
      "--- Starting Training ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HISTORY_SEQ_LEN = 60  # 30秒历史\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "MODEL_SAVE_PATH = \"fidelity_model_best.pth\"\n",
    "NOISE_LEVEL = 0.25\n",
    "\n",
    "\n",
    "# --- 4. 初始化模型并开始训练 ---\n",
    "print(\"\\nInitializing model for training...\")\n",
    "fidelity_model = FidelityModelGated(\n",
    "    feature_dim=6400,\n",
    "    lstm_hidden_dim=256,\n",
    "    raw_cnn_output_dim=6400\n",
    ").to(DEVICE)\n",
    "\n",
    "# 使用BCEWithLogitsLoss，它内置了sigmoid，更稳定\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(fidelity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(fidelity_model, train_loader, criterion, optimizer, DEVICE, noise_level=NOISE_LEVEL)\n",
    "    val_metrics = evaluate(fidelity_model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "        f\"Val Accuracy: {val_metrics['accuracy']:.4f} | \"\n",
    "        f\"Val F1: {val_metrics['f1']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # 保存表现最好的模型（以F1分数为标准）\n",
    "    if val_metrics['f1'] > best_val_f1 or val_metrics['loss'] < best_val_loss:\n",
    "        best_val_f1 = val_metrics['f1']\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        torch.save(fidelity_model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"  -> New best model saved to {MODEL_SAVE_PATH} (F1: {best_val_f1:.4f})\")\n",
    "\n",
    "print(\"--- Training Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

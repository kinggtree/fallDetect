{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eaad61",
   "metadata": {},
   "source": [
    "# Notice\n",
    "训练之前请确保，特征模型训练后得到的 **`feature_model_1dcnn.pth`** 和 **`scaler_50hz_torch.gz`** 这两个文件存在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7145f7",
   "metadata": {},
   "source": [
    "# 导入依赖库，定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a246b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # 每秒有 50 个数据点\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "SEQUENCE_LENGTH = int(TARGET_SAMPLING_RATE_HZ * 4) # 200 samples for 4 seconds at 50Hz\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91559b",
   "metadata": {},
   "source": [
    "# 第 1 步：处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < SEQUENCE_LENGTH: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "# def create_sequences(data_list, label_list, seq_length, step):\n",
    "#     \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "#     # 初始化用于存放最终序列和对应标签的列表\n",
    "#     X, y = [], []\n",
    "#     # 遍历每一次活动试验的数据\n",
    "#     for i, trial_data in enumerate(data_list):\n",
    "#         trial_label = label_list[i]\n",
    "#         # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "#         for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "#             # 截取一个固定长度（seq_length）的片段作为序列\n",
    "#             X.append(trial_data[j:(j + seq_length)])\n",
    "#             # 为这个序列分配对应的标签\n",
    "#             y.append(trial_label)\n",
    "            \n",
    "#     if not X: return np.array([]), np.array([])\n",
    "#     # 将列表转换为Numpy数组后返回\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "# x, y = create_sequences(trial_arrays, trial_labels, SEQUENCE_LENGTH, STEP)\n",
    "# print(f\"The shape of X: {x.shape}, The shape of y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57806b03",
   "metadata": {},
   "source": [
    "## 创建连续的数据流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e947c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_continuous_stream(data_list: list, label_list: list) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    将所有试验数据拼接成一个连续的、无采样的原始数据流，并生成对应的标签流。\n",
    "\n",
    "    参数:\n",
    "    - data_list (list): 一个列表，其中每个元素是一个试验的Numpy数组 (形状为 [n_samples, 11])。\n",
    "    - label_list (list): 一个列表，包含每个试验对应的标签 (0 或 1)。\n",
    "\n",
    "    返回:\n",
    "    - continuous_data (np.ndarray): 拼接后的连续数据流，形状为 [总时间点数, 11]。\n",
    "    - continuous_labels (np.ndarray): 对应的连续标签流，形状为 [总时间点数,]。\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # 为每个试验的每个时间点生成对应的标签\n",
    "    # 例如，如果一个试验有491个时间点，标签是1，我们就生成一个包含491个1的数组\n",
    "    all_trial_labels_expanded = []\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        num_timesteps = trial_data.shape[0]  # 获取该试验的时间点数量\n",
    "        trial_label = label_list[i]\n",
    "        # 使用 np.full 创建一个长度为 num_timesteps，值全为 trial_label 的数组\n",
    "        labels_for_this_trial = np.full(num_timesteps, trial_label, dtype=np.int32)\n",
    "        all_trial_labels_expanded.append(labels_for_this_trial)\n",
    "\n",
    "    # 使用 np.concatenate 将所有试验数据数组沿第一个轴（时间轴）拼接起来\n",
    "    continuous_data = np.concatenate(data_list, axis=0)\n",
    "    \n",
    "    # 同样地，拼接所有扩展后的标签数组\n",
    "    continuous_labels = np.concatenate(all_trial_labels_expanded, axis=0)\n",
    "\n",
    "    return continuous_data, continuous_labels\n",
    "\n",
    "continuous_data, continuous_labels = create_continuous_stream(trial_arrays, trial_labels)\n",
    "print(f\"Continuous data shape: {continuous_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64e0b7",
   "metadata": {},
   "source": [
    "# 截断数据流，仅保留十分之一（测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_data = continuous_data[:len(continuous_data)//10]  # 截断数据流（测试）\n",
    "continuous_labels = continuous_labels[:len(continuous_labels)//10]\n",
    "print(f\"Truncated continuous data shape: {continuous_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6fe3",
   "metadata": {},
   "source": [
    "## 定义 1D-CNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1):\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 200 -> 100\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 100 -> 50\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: 50 -> 25\n",
    "        )\n",
    "        \n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        # 输入维度需要计算: 256 (channels) * 25 (length)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 25, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        标准的前向传播，用于训练和评估\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # Conv1d 需要 (N, C, L) 格式, 所以我们需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        # 因为使用 BCEWithLogitsLoss, 所以不需要在这里加 sigmoid\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        仅用于提取中间特征的函数\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # 同样需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        # 只通过特征提取器\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 输出形状将是 (N, 256, 25)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c20b2f",
   "metadata": {},
   "source": [
    "# 第 2 步：生成连续的特征流和标签流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "# 确保这些参数与模型训练和模拟逻辑一致\n",
    "WINDOW_SECONDS = 4\n",
    "\n",
    "WINDOW_SIZE = SEQUENCE_LENGTH\n",
    "STEP_SIZE = 25 # 步长 (25个点)，即每0.5秒提取一次特征\n",
    "\n",
    "MODEL_PATH = \"feature_model_1dcnn.pth\"\n",
    "SCALER_PATH = \"scaler_50hz_torch.gz\"\n",
    "\n",
    "# 如果已经存在all_features.npy和all_labels.npy，就跳过该单元格\n",
    "if os.path.exists(\"all_features.npy\") and os.path.exists(\"all_labels.npy\"):\n",
    "    print(\"all_features.npy 和 all_labels.npy 已存在，跳过特征提取步骤。\")\n",
    "\n",
    "else:\n",
    "    print(\"all_features.npy 和 all_labels.npy 不存在，开始特征提取步骤。\")\n",
    "    # --- 2. 加载模型和标准化器 ---\n",
    "    print(\"正在加载模型和标准化器...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载模型\n",
    "    model = FeatureModel1DCNN(input_channels=11, num_classes=1).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(f\"模型已从 {MODEL_PATH} 加载\")\n",
    "    else:\n",
    "        print(f\"警告: 在 {MODEL_PATH} 未找到模型文件。将使用随机初始化的模型。\")\n",
    "    model.eval() # 设置为评估模式\n",
    "\n",
    "    # 加载标准化器\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        print(f\"标准化器已从 {SCALER_PATH} 加载\")\n",
    "    else:\n",
    "        # 抛出错误并停止执行\n",
    "        raise FileNotFoundError(f\"ERROR: Standard scaler file not found at '{SCALER_PATH}'. Cannot proceed without it.\")\n",
    "\n",
    "\n",
    "    # --- 3. 批量处理数据并提取特征 ---\n",
    "    print(\"\\n开始批量提取特征...\")\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "\n",
    "    # `trial_arrays` 和 `trial_labels` 变量是从上一个数据加载单元格中获得的\n",
    "    # 遍历每一次试验的数据\n",
    "    for i in range(len(continuous_data) - WINDOW_SIZE + 1):\n",
    "        trial_label = continuous_labels[i]\n",
    "\n",
    "        window = continuous_data[i : i + WINDOW_SIZE]\n",
    "        \n",
    "        # 2. 预处理窗口数据 (标准化 -> 转换为Tensor)\n",
    "        scaled_window = scaler.transform(window)\n",
    "        window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 3. 从模型中提取特征\n",
    "        with torch.no_grad(): # 关闭梯度计算以加速\n",
    "            features = model.extract_features(window_tensor)\n",
    "        \n",
    "        # 4. 将特征扁平化并添加到列表中\n",
    "        flattened_features = features.cpu().numpy().flatten()\n",
    "        all_features_list.append(flattened_features)\n",
    "        \n",
    "        # 5. 将该窗口对应的标签添加到列表中\n",
    "        all_labels_list.append(trial_label)\n",
    "\n",
    "    print(f\"处理完成！共处理了 {len(trial_arrays)} 次试验，生成了 {len(all_features_list)} 个特征向量。\")\n",
    "\n",
    "    # --- 4. 保存最终的数据集 ---\n",
    "    if all_features_list:\n",
    "        # 将列表转换为Numpy数组\n",
    "        final_features = np.array(all_features_list)\n",
    "        final_labels = np.array(all_labels_list)\n",
    "\n",
    "        # 保存数组到.npy文件\n",
    "        np.save(\"all_features.npy\", final_features)\n",
    "        np.save(\"all_labels.npy\", final_labels)\n",
    "\n",
    "        print(f\"\\n数据集已成功保存:\")\n",
    "        print(f\"  - 特征文件: all_features.npy, 形状: {final_features.shape}\")\n",
    "        print(f\"  - 标签文件: all_labels.npy, 形状: {final_labels.shape}\")\n",
    "    else:\n",
    "        print(\"\\n未能生成任何特征，未创建文件。\")\n",
    "\n",
    "    # 释放内存，清理变量\n",
    "    del model\n",
    "    del scaler\n",
    "    del all_features_list\n",
    "    del all_labels_list\n",
    "    del final_features\n",
    "    del final_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ba75e",
   "metadata": {},
   "source": [
    "## 加载 npy 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6e9c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading foundational data streams...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "HISTORY_LEN = 60    # 30 秒内特征的个数（两个连续特征之间的时间间隔是 0.5 秒）\n",
    "BATCH_SIZE = 32\n",
    "AVAILABILITY_RATIO = 0.3\n",
    "\n",
    "# --- 2. 加载或模拟基础数据流 ---\n",
    "print(\"Loading foundational data streams...\")\n",
    "# 【用户待办】您需要加载您真实的数据\n",
    "try:\n",
    "    all_features = np.load(\"all_features.npy\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: `all_features.npy` not found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f7f19",
   "metadata": {},
   "source": [
    "# 第 3 步：生成场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7004a503",
   "metadata": {},
   "source": [
    "## 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualFidelityDataset_OnTheFly(Dataset):\n",
    "    \"\"\"\n",
    "    一个内存高效的数据集类，它在被请求时即时生成每个训练样本。\n",
    "    \"\"\"\n",
    "    def __init__(self, all_features, continuous_raw_data, continuous_labels,\n",
    "                 window_size, history_len, availability_ratio):\n",
    "        \n",
    "        super().__init__()\n",
    "        print(\"--- Initializing On-the-Fly Dataset ---\")\n",
    "\n",
    "        # 1. 存储基础数据和参数\n",
    "        self.window_size = window_size\n",
    "        self.history_len = history_len\n",
    "        self.availability_ratio = availability_ratio\n",
    "        \n",
    "        # 2. 对齐数据流 (这一步在初始化时执行一次即可)\n",
    "        print(\"Aligning streams...\")\n",
    "        offset = self.window_size - 1\n",
    "        \n",
    "        self.aligned_lfs = all_features\n",
    "        self.aligned_labels = continuous_labels[offset : offset + len(all_features)]\n",
    "        \n",
    "        # 将原始数据也存储起来，以便在 __getitem__ 中切片\n",
    "        # 注意：这里我们只存储原始的连续数据流，而不是生成所有窗口\n",
    "        self.continuous_raw_data = continuous_raw_data\n",
    "        \n",
    "        self.num_features = len(self.aligned_lfs)\n",
    "        print(f\"Initialization complete. Total possible scenarios: {self.__len__()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # 可生成的总样本数\n",
    "        return self.num_features - self.history_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # DataLoader 请求第 idx 个样本时，此方法被调用\n",
    "        \n",
    "        # 1. 计算当前样本在对齐流中的结束时间点\n",
    "        t = idx + self.history_len - 1\n",
    "        \n",
    "        # 2. 提取低保真历史特征序列\n",
    "        feature_sequence = self.aligned_lfs[t - self.history_len + 1 : t + 1]\n",
    "        \n",
    "        # 3. 提取标签\n",
    "        label = self.aligned_labels[t]\n",
    "        \n",
    "        # 4. 即时构建并稀疏化高保真历史原始数据序列\n",
    "        hfs_history_list = []\n",
    "        for i in range(self.history_len):\n",
    "            historical_time_index_in_lfs = t - self.history_len + 1 + i\n",
    "            \n",
    "            if random.random() < self.availability_ratio:\n",
    "                # 从 *最原始的* continuous_raw_data 中截取窗口\n",
    "                start_index = historical_time_index_in_lfs\n",
    "                end_index = start_index + self.window_size\n",
    "                raw_window = self.continuous_raw_data[start_index:end_index]\n",
    "                hfs_history_list.append(raw_window)\n",
    "            else:\n",
    "                # 用全零窗口作为占位符\n",
    "                hfs_history_list.append(np.zeros((self.window_size, 11)))\n",
    "        \n",
    "        # 5. 将窗口列表堆叠成一个大的Numpy数组\n",
    "        imputed_raw_sequence = np.stack(hfs_history_list, axis=0)\n",
    "        \n",
    "        # 6. 转换为Tensor并返回\n",
    "        return (\n",
    "            torch.tensor(feature_sequence, dtype=torch.float32),\n",
    "            torch.tensor(imputed_raw_sequence, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# # --- 3. 初始化“即时生成”数据集 ---\n",
    "# full_dataset = ContextualFidelityDataset_OnTheFly(\n",
    "#     all_features=all_features,\n",
    "#     continuous_raw_data=continuous_data,\n",
    "#     continuous_labels=continuous_labels,\n",
    "#     window_size=WINDOW_SIZE,\n",
    "#     history_len=HISTORY_LEN,\n",
    "#     availability_ratio=AVAILABILITY_RATIO\n",
    "# )\n",
    "\n",
    "# # --- 4. 划分训练、验证集 (通过划分索引) ---\n",
    "# print(\"\\nSplitting data into training and validation sets by indices...\")\n",
    "# dataset_size = len(full_dataset)\n",
    "# indices = list(range(dataset_size))\n",
    "# labels_for_stratify = full_dataset.aligned_labels[HISTORY_LEN - 1:] # 获取用于分层的标签\n",
    "\n",
    "# train_indices, val_indices = train_test_split(\n",
    "#     indices, test_size=0.2, random_state=42, stratify=labels_for_stratify\n",
    "# )\n",
    "\n",
    "# # 使用 PyTorch 的 Subset 来创建基于索引的子数据集\n",
    "# train_dataset = Subset(full_dataset, train_indices)\n",
    "# val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "# print(f\"Training set size: {len(train_dataset)}\")\n",
    "# print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# # 创建DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "# --- 5. 健全性检查 ---\n",
    "# print(\"\\n--- DataLoader Sanity Check ---\")\n",
    "# batch_feature_seq, batch_raw_seq, batch_labels = next(iter(train_loader))\n",
    "\n",
    "# print(f\"Feature sequence batch shape: {batch_feature_seq.shape}\")\n",
    "# print(f\"Raw data sequence batch shape: {batch_raw_seq.shape}\")\n",
    "# print(f\"Labels batch shape: {batch_labels.shape}\")\n",
    "\n",
    "# print(\"\\nMemory-efficient dataset pipeline is ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643f042",
   "metadata": {},
   "source": [
    "## 保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0296716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def generate_and_save_scenarios_as_chunks(\n",
    "    all_features, continuous_raw_data, continuous_labels,\n",
    "    window_size, history_len, availability_ratio,\n",
    "    chunk_size_gb=1.0, output_dir=\"precomputed_dataset\", num_epochs_to_generate=1):\n",
    "    \n",
    "    print(\"--- Starting Chunked Dataset Generation ---\")\n",
    "    \n",
    "    # 对齐数据流\n",
    "    offset = window_size - 1\n",
    "    aligned_labels = continuous_labels[offset : offset + len(all_features)]\n",
    "    num_features = len(all_features)\n",
    "    \n",
    "    # 计算每个样本的大约大小 (MB)\n",
    "    sample_size_bytes = (history_len * window_size * 11 * 4) + (history_len * 6400 * 4)\n",
    "    sample_size_mb = sample_size_bytes / (1024**2)\n",
    "    \n",
    "    # 计算每个文件块应包含多少个样本\n",
    "    chunk_size_mb = chunk_size_gb * 1024\n",
    "    samples_per_chunk = math.floor(chunk_size_mb / sample_size_mb)\n",
    "    print(f\"Each sample is ~{sample_size_mb:.2f} MB. Each chunk will contain {samples_per_chunk} samples.\")\n",
    "\n",
    "    # --- 核心改动：为每个epoch生成一套独立的、随机的数据 ---\n",
    "    for epoch_idx in range(num_epochs_to_generate):\n",
    "        epoch_dir = os.path.join(output_dir, f\"epoch_{epoch_idx}\")\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        print(f\"\\nGenerating data for Epoch {epoch_idx}...\")\n",
    "\n",
    "        scenarios_chunk = []\n",
    "        chunk_idx = 0\n",
    "        \n",
    "        # 遍历所有可能的样本\n",
    "        for t in range(history_len - 1, num_features):\n",
    "            feature_sequence = all_features[t - history_len + 1 : t + 1]\n",
    "            hfs_history_list = []\n",
    "            for i in range(history_len):\n",
    "                historical_time_index_in_lfs = t - history_len + 1 + i\n",
    "                if random.random() < availability_ratio:\n",
    "                    start_index = historical_time_index_in_lfs\n",
    "                    end_index = start_index + window_size\n",
    "                    raw_window = continuous_raw_data[start_index:end_index]\n",
    "                    hfs_history_list.append(raw_window)\n",
    "                else:\n",
    "                    hfs_history_list.append(np.zeros((window_size, 11)))\n",
    "            \n",
    "            imputed_raw_sequence = np.stack(hfs_history_list, axis=0)\n",
    "            label = aligned_labels[t]\n",
    "            \n",
    "            # 使用torch tensor来保存，读取更快\n",
    "            scenarios_chunk.append((\n",
    "                torch.tensor(feature_sequence, dtype=torch.float32),\n",
    "                torch.tensor(imputed_raw_sequence, dtype=torch.float32),\n",
    "                torch.tensor(label, dtype=torch.float32)\n",
    "            ))\n",
    "            \n",
    "            # 如果当前块满了，就保存\n",
    "            if len(scenarios_chunk) >= samples_per_chunk:\n",
    "                chunk_path = os.path.join(epoch_dir, f\"chunk_{chunk_idx}.pt\")\n",
    "                print(f\"  Saving {len(scenarios_chunk)} samples to {chunk_path}...\")\n",
    "                torch.save(scenarios_chunk, chunk_path)\n",
    "                \n",
    "                # 重置\n",
    "                scenarios_chunk = []\n",
    "                chunk_idx += 1\n",
    "        \n",
    "        # 保存最后一个不满的块\n",
    "        if scenarios_chunk:\n",
    "            chunk_path = os.path.join(epoch_dir, f\"chunk_{chunk_idx}.pt\")\n",
    "            print(f\"  Saving final {len(scenarios_chunk)} samples to {chunk_path}...\")\n",
    "            torch.save(scenarios_chunk, chunk_path)\n",
    "\n",
    "    print(\"\\nDataset generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e18129e",
   "metadata": {},
   "source": [
    "## 执行生成并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29703c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_and_save_scenarios_as_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(BASE_DATA_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# --- 运行生成器 ---\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mgenerate_and_save_scenarios_as_chunks\u001b[49m(\n\u001b[0;32m     11\u001b[0m     all_features, continuous_data, continuous_labels,\n\u001b[0;32m     12\u001b[0m     window_size\u001b[38;5;241m=\u001b[39mWINDOW_SIZE, history_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m, availability_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m     13\u001b[0m     chunk_size_gb\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[0;32m     14\u001b[0m     num_epochs_to_generate\u001b[38;5;241m=\u001b[39mNUM_PRECOMPUTED_EPOCHS, \u001b[38;5;66;03m# 预生成5个不同随机版本的epoch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mBASE_DATA_DIR\n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_and_save_scenarios_as_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "all_features = np.load(\"all_features.npy\")\n",
    "num_features = len(all_features)\n",
    "\n",
    "NUM_PRECOMPUTED_EPOCHS = 1 # 您预生成的epoch数据版本数量\n",
    "# 保存到D盘\n",
    "BASE_DATA_DIR = \"D:/MobiFall_Precomputed\"\n",
    "os.makedirs(BASE_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# --- 运行生成器 ---\n",
    "generate_and_save_scenarios_as_chunks(\n",
    "    all_features, continuous_data, continuous_labels,\n",
    "    window_size=WINDOW_SIZE, history_len=60, availability_ratio=0.3,\n",
    "    chunk_size_gb=1.0, \n",
    "    num_epochs_to_generate=NUM_PRECOMPUTED_EPOCHS, # 预生成5个不同随机版本的epoch\n",
    "    output_dir=BASE_DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0eeb9c",
   "metadata": {},
   "source": [
    "## 读取数据块的 Dataset 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd87e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedChunkDataset(Dataset):\n",
    "    def __init__(self, epoch_dir):\n",
    "        print(f\"Initializing dataset from precomputed chunks in {epoch_dir}...\")\n",
    "        self.epoch_dir = epoch_dir\n",
    "        \n",
    "        chunk_files = sorted(\n",
    "            [f for f in os.listdir(epoch_dir) if f.endswith('.pt')],\n",
    "            key=lambda x: int(x.split('_')[1].split('.')[0])\n",
    "        )\n",
    "        \n",
    "        self.chunk_paths = [os.path.join(epoch_dir, f) for f in chunk_files]\n",
    "        \n",
    "        # 扫描所有块，建立索引\n",
    "        self.index_map = []\n",
    "        self.chunk_sizes = []\n",
    "        total_samples = 0\n",
    "        for i, path in enumerate(self.chunk_paths):\n",
    "            # 这是一个快速但不精确的获取长度的方法，下面会用精确的\n",
    "            # num_samples_in_chunk = len(torch.load(path)) # 这样会加载整个文件\n",
    "            # 为了快速初始化，我们可以先假设或存储元数据，但这里为了简单，我们还是加载一次\n",
    "            num_samples_in_chunk = len(torch.load(path))\n",
    "            self.chunk_sizes.append(num_samples_in_chunk)\n",
    "            for j in range(num_samples_in_chunk):\n",
    "                self.index_map.append((i, j)) # (块索引, 块内索引)\n",
    "            total_samples += num_samples_in_chunk\n",
    "            \n",
    "        self._len = total_samples\n",
    "        \n",
    "        # 用于缓存最近加载的块，避免频繁读盘\n",
    "        self.last_loaded_chunk_idx = -1\n",
    "        self.cached_chunk = None\n",
    "        print(f\"Found {len(self.chunk_paths)} chunks, total {self._len} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_idx, in_chunk_idx = self.index_map[idx]\n",
    "        \n",
    "        # 如果请求的块不在缓存中，则加载它\n",
    "        if chunk_idx != self.last_loaded_chunk_idx:\n",
    "            # print(f\"Loading chunk {chunk_idx}...\") # for debugging\n",
    "            self.cached_chunk = torch.load(self.chunk_paths[chunk_idx])\n",
    "            self.last_loaded_chunk_idx = chunk_idx\n",
    "            \n",
    "        return self.cached_chunk[in_chunk_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fa41c",
   "metadata": {},
   "source": [
    "## 时间分布CNN\n",
    "它的作用是将一个作用于单个样本的模块（如CNN）应用到一个序列中的每一个元素上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062ba0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状: (batch_size, time_steps, C, H, W) 或 (batch_size, time_steps, features...)\n",
    "        # 我们这里是 (batch_size, 60, 200, 11)\n",
    "        \n",
    "        batch_size, time_steps = x.size(0), x.size(1)\n",
    "        \n",
    "        # 1. 合并 batch 和 time 维度\n",
    "        # (B, T, C, F) -> (B * T, C, F)\n",
    "        # 我们的输入是 (B, 60, 200, 11)，需要先 permute\n",
    "        x = x.permute(0, 1, 3, 2) # -> (B, 60, 11, 200)\n",
    "        x_reshape = x.contiguous().view(batch_size * time_steps, x.size(2), x.size(3))\n",
    "        # -> (B * 60, 11, 200)\n",
    "\n",
    "        # 2. 应用模块\n",
    "        y = self.module(x_reshape)\n",
    "        \n",
    "        # y 的形状是 (B * 60, output_features)\n",
    "        \n",
    "        # 3. 恢复 batch 和 time 维度\n",
    "        y = y.view(batch_size, time_steps, y.size(-1))\n",
    "        # -> (B, 60, output_features)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33828b6b",
   "metadata": {},
   "source": [
    "## 交叉注意力模块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cd000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query (来自LFS): (Batch, SeqLen, query_dim)\n",
    "        # key/value (来自HFS): (Batch, SeqLen, key_dim)\n",
    "        \n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(key)\n",
    "        V = self.value_layer(value)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 应用权重\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763c075",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7194fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_data_cnn():\n",
    "    \"\"\"创建一个用于处理原始传感器数据的1D-CNN模块。\"\"\"\n",
    "    raw_data_processor = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=11, out_channels=64, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(64),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    return raw_data_processor\n",
    "\n",
    "\n",
    "class ContextualFidelityModel(nn.Module):\n",
    "    def __init__(self, feature_dim, lstm_hidden_dim, raw_cnn_output_dim, num_classes=1):\n",
    "        super(ContextualFidelityModel, self).__init__()\n",
    "\n",
    "        # --- 分支一：高保真原始数据流处理器 ---\n",
    "        raw_cnn = create_raw_data_cnn()\n",
    "        self.hfs_processor = TimeDistributed(raw_cnn)\n",
    "\n",
    "        # --- 分支二：低保真特征流处理器 ---\n",
    "        self.lfs_processor = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "        # --- 融合模块：交叉注意力 ---\n",
    "        # query 来自 lfs_processor (lstm_hidden_dim)\n",
    "        # key/value 来自 hfs_processor (raw_cnn_output_dim)\n",
    "        self.cross_attention = CrossAttention(\n",
    "            query_dim=lstm_hidden_dim,\n",
    "            key_dim=raw_cnn_output_dim,\n",
    "            hidden_dim=lstm_hidden_dim # 通常设置为与query_dim一致\n",
    "        )\n",
    "        \n",
    "        # --- 后融合处理器与分类器 ---\n",
    "        # 将 LSTM 的输出和注意力机制的输出结合起来\n",
    "        self.post_fusion_processor = nn.LSTM(\n",
    "            input_size=lstm_hidden_dim * 2, # Concatenated input\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, imputed_raw_sequence):\n",
    "        # feature_sequence: (B, 60, 6400)\n",
    "        # imputed_raw_sequence: (B, 60, 200, 11)\n",
    "\n",
    "        # 1. 并行处理两条流\n",
    "        lfs_output, _ = self.lfs_processor(feature_sequence) # -> (B, 60, lstm_hidden_dim)\n",
    "        hfs_output = self.hfs_processor(imputed_raw_sequence) # -> (B, 60, raw_cnn_output_dim)\n",
    "\n",
    "        # 2. 交叉注意力融合\n",
    "        # lfs_output 作为 Query，去查询 hfs_output\n",
    "        attention_context = self.cross_attention(\n",
    "            query=lfs_output, \n",
    "            key=hfs_output, \n",
    "            value=hfs_output\n",
    "        ) # -> (B, 60, lstm_hidden_dim)\n",
    "        \n",
    "        # 3. 结合 LFS 输出和注意力上下文\n",
    "        combined_features = torch.cat([lfs_output, attention_context], dim=-1)\n",
    "        # -> (B, 60, lstm_hidden_dim * 2)\n",
    "\n",
    "        # 4. 后融合处理与最终裁决\n",
    "        final_sequence, (h_n, _) = self.post_fusion_processor(combined_features)\n",
    "        \n",
    "        # 使用序列的最后一个时间点的输出进行分类\n",
    "        last_step_output = final_sequence[:, -1, :]\n",
    "        logits = self.classifier(last_step_output)\n",
    "        \n",
    "        # 状态特征依然是最后一个LSTM的隐藏状态\n",
    "        state_feature = h_n.squeeze(0) # -> (B, lstm_hidden_dim)\n",
    "\n",
    "        return logits, state_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99ba2",
   "metadata": {},
   "source": [
    "## 训练与评估函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b710bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, noise_level=0.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # 循环变量名修改，以匹配DataLoader的输出\n",
    "    for feature_seq, imputed_raw_seq, labels in dataloader:\n",
    "        feature_seq = feature_seq.to(device)\n",
    "        # 将新的 imputed_raw_seq 移动到设备\n",
    "        imputed_raw_seq = imputed_raw_seq.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 更新模型调用方式，并只接收两个返回值\n",
    "        logits, _ = model(feature_seq, imputed_raw_seq)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * feature_seq.size(0)\n",
    "    \n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 循环变量名修改\n",
    "        for feature_seq, imputed_raw_seq, labels in dataloader:\n",
    "            feature_seq = feature_seq.to(device)\n",
    "            # 将新的 imputed_raw_seq 移动到设备\n",
    "            imputed_raw_seq = imputed_raw_seq.to(device)\n",
    "            labels_for_loss = labels.to(device).unsqueeze(1) # 用于计算loss\n",
    "\n",
    "            # 更新模型调用方式，并只接收两个返回值\n",
    "            logits, _ = model(feature_seq, imputed_raw_seq)\n",
    "            \n",
    "            loss = criterion(logits, labels_for_loss)\n",
    "            total_loss += loss.item() * feature_seq.size(0)\n",
    "            \n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            # 注意：用于指标计算的标签不需要 unsqueeze\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032eaed",
   "metadata": {},
   "source": [
    "# 第 6 步：训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea2f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model for training...\n",
      "\n",
      "===== Epoch 1/10 =====\n",
      "Loading data from precomputed version: epoch_0\n",
      "Initializing dataset from precomputed chunks in D:/MobiFall_Precomputed\\epoch_0...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HISTORY_SEQ_LEN = 60  # 30秒历史\n",
    "# BATCH_SIZE 见上面定义\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "MODEL_SAVE_PATH = \"fidelity_model_best.pth\"\n",
    "NOISE_LEVEL = 0.25\n",
    "\n",
    "\n",
    "# --- 4. 初始化模型并开始训练 ---\n",
    "print(\"\\nInitializing model for training...\")\n",
    "fidelity_model = ContextualFidelityModel(\n",
    "    feature_dim=6400,\n",
    "    lstm_hidden_dim=256,\n",
    "    raw_cnn_output_dim=6400\n",
    ").to(DEVICE)\n",
    "\n",
    "# 使用BCEWithLogitsLoss，它内置了sigmoid，更稳定\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(fidelity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. 训练主循环 ---\n",
    "for epoch in range(EPOCHS):\n",
    "    # 轮换使用预生成的数据集，实现更好的随机性\n",
    "    epoch_to_load = epoch % NUM_PRECOMPUTED_EPOCHS\n",
    "    epoch_data_dir = os.path.join(BASE_DATA_DIR, f\"epoch_{epoch_to_load}\")\n",
    "    \n",
    "    print(f\"\\n===== Epoch {epoch+1}/{EPOCHS} =====\")\n",
    "    print(f\"Loading data from precomputed version: epoch_{epoch_to_load}\")\n",
    "\n",
    "    # a. 为当前epoch加载完整的数据集\n",
    "    full_dataset = PrecomputedChunkDataset(epoch_data_dir)\n",
    "\n",
    "    # b. 划分训练集和验证集的索引\n",
    "    # 注意：由于数据已预先生成并分块，为了快速初始化，我们不再进行分层抽样(stratify)。\n",
    "    # 对于大数据集，简单的随机划分通常也能保证训练集和验证集中类别分布大致均衡。\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    # c. 使用PyTorch的Subset创建数据集\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "    # d. 创建DataLoader\n",
    "    # pin_memory=True 可以在数据从CPU传到GPU时提速\n",
    "    # num_workers > 0 可以使用多进程加载数据，进一步提升效率\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # e. 执行训练和评估\n",
    "    print(\"Starting training for this epoch...\")\n",
    "    train_loss = train_one_epoch(fidelity_model, train_loader, criterion, optimizer, DEVICE, noise_level=0.1)\n",
    "    val_metrics = evaluate(fidelity_model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} results: Train Loss: {train_loss:.4f}, Val F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

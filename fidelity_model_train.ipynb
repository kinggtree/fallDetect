{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eaad61",
   "metadata": {},
   "source": [
    "# Notice\n",
    "训练之前请确保，特征模型训练后得到的 **`feature_model_1dcnn.pth`** 和 **`scaler_50hz_torch.gz`** 这两个文件存在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7145f7",
   "metadata": {},
   "source": [
    "# 导入依赖库，定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a246b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "SEQUENCE_LENGTH = int(TARGET_SAMPLING_RATE_HZ * 4) # 200 samples for 4 seconds at 50Hz\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * 1)          # 50 samples for 1 second step at 50Hz\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91559b",
   "metadata": {},
   "source": [
    "# 第 1 步：处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < SEQUENCE_LENGTH: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "def create_sequences(data_list, label_list, seq_length, step):\n",
    "    \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "    # 初始化用于存放最终序列和对应标签的列表\n",
    "    X, y = [], []\n",
    "    # 遍历每一次活动试验的数据\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        trial_label = label_list[i]\n",
    "        # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "        for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "            # 截取一个固定长度（seq_length）的片段作为序列\n",
    "            X.append(trial_data[j:(j + seq_length)])\n",
    "            # 为这个序列分配对应的标签\n",
    "            y.append(trial_label)\n",
    "            \n",
    "    if not X: return np.array([]), np.array([])\n",
    "    # 将列表转换为Numpy数组后返回\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "X_sequences, y_sequences = create_sequences(trial_arrays, trial_labels, SEQUENCE_LENGTH, STEP)\n",
    "print(f\"Created {X_sequences.shape} sequences.\")\n",
    "print(f\"Created {y_sequences.shape} labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6fe3",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1):\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 200 -> 100\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 100 -> 50\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: 50 -> 25\n",
    "        )\n",
    "        \n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        # 输入维度需要计算: 256 (channels) * 25 (length)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 25, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        标准的前向传播，用于训练和评估\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # Conv1d 需要 (N, C, L) 格式, 所以我们需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        # 因为我们使用 BCEWithLogitsLoss, 所以不需要在这里加 sigmoid\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        仅用于提取中间特征的函数\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # 同样需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        # 只通过特征提取器\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 输出形状将是 (N, 256, 25)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c20b2f",
   "metadata": {},
   "source": [
    "# 第 2 步：生成连续的特征流和标签流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "# 确保这些参数与您的模型训练和模拟逻辑一致\n",
    "SAMPLING_RATE_HZ = 50\n",
    "WINDOW_SECONDS = 4\n",
    "STEP_SECONDS = 0.5 # 0.5秒步长\n",
    "\n",
    "WINDOW_SIZE = SAMPLING_RATE_HZ * WINDOW_SECONDS # 窗口大小 (200个点)\n",
    "STEP_SIZE = int(SAMPLING_RATE_HZ * STEP_SECONDS) # 步长 (25个点)\n",
    "\n",
    "MODEL_PATH = \"feature_model_1dcnn.pth\"\n",
    "SCALER_PATH = \"scaler_50hz_torch.gz\"\n",
    "\n",
    "# --- 2. 加载模型和标准化器 ---\n",
    "print(\"正在加载模型和标准化器...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载模型\n",
    "model = FeatureModel1DCNN(input_channels=11, num_classes=1).to(device)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f\"模型已从 {MODEL_PATH} 加载\")\n",
    "else:\n",
    "    print(f\"警告: 在 {MODEL_PATH} 未找到模型文件。将使用随机初始化的模型。\")\n",
    "model.eval() # 设置为评估模式\n",
    "\n",
    "# 加载标准化器\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(f\"标准化器已从 {SCALER_PATH} 加载\")\n",
    "else:\n",
    "    print(f\"警告: 在 {SCALER_PATH} 未找到标准化器文件。这将导致数据预处理不正确。\")\n",
    "    # 创建一个虚拟的scaler以避免代码崩溃，但这对于实际应用是错误的\n",
    "    scaler = StandardScaler()\n",
    "    if 'trial_arrays' in locals() and len(trial_arrays) > 0:\n",
    "        scaler.fit(np.vstack(trial_arrays)) # 使用所有数据拟合一个临时的scaler\n",
    "\n",
    "\n",
    "# --- 3. 批量处理数据并提取特征 ---\n",
    "print(\"\\n开始批量提取特征...\")\n",
    "all_features_list = []\n",
    "all_labels_list = []\n",
    "\n",
    "# `trial_arrays` 和 `trial_labels` 变量是从上一个数据加载单元格中获得的\n",
    "# 遍历每一次试验的数据\n",
    "for i, trial_data in enumerate(trial_arrays):\n",
    "    trial_label = trial_labels[i]\n",
    "    \n",
    "    # 在当前试验数据上应用滑动窗口\n",
    "    for j in range(0, len(trial_data) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        # 1. 截取一个窗口的数据\n",
    "        window_data = trial_data[j : j + WINDOW_SIZE]\n",
    "        \n",
    "        # 2. 预处理窗口数据 (标准化 -> 转换为Tensor)\n",
    "        scaled_window = scaler.transform(window_data)\n",
    "        window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # 3. 从模型中提取特征\n",
    "        with torch.no_grad(): # 关闭梯度计算以加速\n",
    "            features = model.extract_features(window_tensor)\n",
    "        \n",
    "        # 4. 将特征扁平化并添加到列表中\n",
    "        flattened_features = features.cpu().numpy().flatten()\n",
    "        all_features_list.append(flattened_features)\n",
    "        \n",
    "        # 5. 将该窗口对应的标签添加到列表中\n",
    "        all_labels_list.append(trial_label)\n",
    "\n",
    "print(f\"处理完成！共处理了 {len(trial_arrays)} 次试验，生成了 {len(all_features_list)} 个特征向量。\")\n",
    "\n",
    "# --- 4. 保存最终的数据集 ---\n",
    "if all_features_list:\n",
    "    # 将列表转换为Numpy数组\n",
    "    final_features = np.array(all_features_list)\n",
    "    final_labels = np.array(all_labels_list)\n",
    "\n",
    "    # 保存数组到.npy文件\n",
    "    np.save(\"all_features.npy\", final_features)\n",
    "    np.save(\"all_labels.npy\", final_labels)\n",
    "\n",
    "    print(f\"\\n数据集已成功保存:\")\n",
    "    print(f\"  - 特征文件: all_features.npy, 形状: {final_features.shape}\")\n",
    "    print(f\"  - 标签文件: all_labels.npy, 形状: {final_labels.shape}\")\n",
    "else:\n",
    "    print(\"\\n未能生成任何特征，未创建文件。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f7f19",
   "metadata": {},
   "source": [
    "# 第 3 步：稀疏化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"正在模拟生成稀疏的原始数据...\")\n",
    "\n",
    "# --- 1. 定义稀疏度 ---\n",
    "# p 是在每个时间点，我们决定保留其对应原始数据窗口的概率。\n",
    "# 例如，p = 0.2 意味着我们期望最终能保留大约20%的原始数据片段。\n",
    "p_sparsity = 0.2\n",
    "\n",
    "# --- 2. 初始化存储结构和计数器 ---\n",
    "# 我们使用一个字典来存储结果。\n",
    "# - Key: 特征在完整序列中的时间点索引 (i)。\n",
    "# - Value: 对应的原始4秒传感器数据窗口 (形状为 [200, 11])。\n",
    "sparse_raw_data = {}\n",
    "\n",
    "# 这个计数器 `feature_index` 用于追踪我们在完整特征流中的确切位置 (时间点 i)，\n",
    "# 以确保 key 的正确性。\n",
    "feature_index = 0\n",
    "\n",
    "# --- 3. 遍历所有数据窗口并根据概率进行采样 ---\n",
    "# 下面的循环逻辑必须与上一步生成 all_features.npy 时的逻辑完全一致，\n",
    "# 这样才能保证时间点索引 `feature_index` 能够与特征流中的每一个特征一一对应。\n",
    "#\n",
    "# (请确保 `trial_arrays`, `WINDOW_SIZE`, `STEP_SIZE` 这些变量在之前的单元格中已经被定义)\n",
    "\n",
    "print(f\"使用稀疏度 p = {p_sparsity} 开始采样...\")\n",
    "\n",
    "for trial_data in trial_arrays:\n",
    "    for j in range(0, len(trial_data) - WINDOW_SIZE + 1, STEP_SIZE):\n",
    "        \n",
    "        # 对每个时间点，生成一个 [0, 1) 之间的随机数\n",
    "        if random.random() < p_sparsity:\n",
    "            # 如果随机数小于我们设定的概率 p，\n",
    "            # 就获取这个窗口的原始数据并存入字典。\n",
    "            raw_window_data = trial_data[j : j + WINDOW_SIZE]\n",
    "            sparse_raw_data[feature_index] = raw_window_data\n",
    "        \n",
    "        # 关键：无论当前窗口的数据是否被保留，时间点索引都必须加 1，\n",
    "        # 以确保它始终代表在完整、非稀疏的特征流中的位置。\n",
    "        feature_index += 1\n",
    "\n",
    "# --- 4. 打印结果摘要 ---\n",
    "total_windows = feature_index\n",
    "kept_windows_count = len(sparse_raw_data)\n",
    "effective_keep_rate = (kept_windows_count / total_windows) * 100 if total_windows > 0 else 0\n",
    "\n",
    "print(\"\\n稀疏数据模拟完成！\")\n",
    "print(f\"总计遍历的窗口数 (即总特征数): {total_windows}\")\n",
    "print(f\"实际保留的原始数据窗口数: {kept_windows_count}\")\n",
    "print(f\"有效保留率: {effective_keep_rate:.2f}% (理论值为 {p_sparsity*100}%)\")\n",
    "\n",
    "# 打印一些信息来验证我们的数据结构\n",
    "if kept_windows_count > 0:\n",
    "    # 获取并打印前几个被保留下来的窗口的时间点索引\n",
    "    example_indices = list(sparse_raw_data.keys())[:5]\n",
    "    print(f\"\\n字典中存储的示例索引 (前5个): {example_indices}\")\n",
    "    \n",
    "    # 验证第一个被保留的数据的形状\n",
    "    first_key = example_indices[0]\n",
    "    print(f\"例如，在时间点 {first_key} 保存的数据，其形状为: {sparse_raw_data[first_key].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fa41c",
   "metadata": {},
   "source": [
    "## 1D-CNN模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ba0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_data_cnn():\n",
    "    \"\"\"创建一个用于处理原始传感器数据的1D-CNN模块。\"\"\"\n",
    "    raw_data_processor = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=11, out_channels=64, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(64),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    return raw_data_processor\n",
    "\n",
    "class FidelityModelGated(nn.Module):\n",
    "    def __init__(self, feature_dim, lstm_hidden_dim, raw_cnn_output_dim, num_classes=1):\n",
    "        super(FidelityModelGated, self).__init__()\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.raw_cnn_output_dim = raw_cnn_output_dim\n",
    "        self.feature_lstm = nn.LSTM(\n",
    "            input_size=feature_dim, hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2, batch_first=True, dropout=0.5\n",
    "        )\n",
    "        self.raw_data_cnn = create_raw_data_cnn()\n",
    "        self.gating_layer = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim + self.raw_cnn_output_dim, lstm_hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(lstm_hidden_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "        self.raw_transform = nn.Linear(self.raw_cnn_output_dim, self.lstm_hidden_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64), nn.ReLU(),\n",
    "            nn.Dropout(0.5), nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, raw_data=None):\n",
    "        lstm_outputs, (h_n, c_n) = self.feature_lstm(feature_sequence)\n",
    "        lstm_last_output = lstm_outputs[:, -1, :]\n",
    "        if raw_data is not None and raw_data.nelement() > 0: # 确保传入的不是空tensor\n",
    "            raw_data = raw_data.permute(0, 2, 1)\n",
    "            v_raw = self.raw_data_cnn(raw_data)\n",
    "        else:\n",
    "            v_raw = torch.zeros(feature_sequence.size(0), self.raw_cnn_output_dim, device=feature_sequence.device)\n",
    "\n",
    "        combined_for_gate = torch.cat((lstm_last_output, v_raw), dim=1)\n",
    "        gate = self.gating_layer(combined_for_gate)\n",
    "        transformed_v_raw = self.raw_transform(v_raw)\n",
    "        fused_vector = lstm_last_output + gate * torch.tanh(transformed_v_raw)\n",
    "        logits = self.classifier(fused_vector)\n",
    "        state_feature = h_n[-1, :, :].squeeze(0)\n",
    "        return logits, state_feature, gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314320e9",
   "metadata": {},
   "source": [
    "# 第 4 步：组装最终的训练样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_training_samples(all_features, all_labels, sparse_raw_data, history_seq_len=60):\n",
    "    \"\"\"\n",
    "    根据特征流、标签流和稀疏原始数据，组装成最终的训练样本列表。\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Assembling final training samples...\")\n",
    "    X_sequences, X_raw, Y_labels = [], [], []\n",
    "\n",
    "    # 从第一个可以构成完整历史序列的特征点开始遍历\n",
    "    for i in range(history_seq_len - 1, len(all_features)):\n",
    "        # 1. 提取历史特征序列\n",
    "        feature_seq = all_features[i - history_seq_len + 1 : i + 1]\n",
    "        X_sequences.append(feature_seq)\n",
    "\n",
    "        # 2. 提取稀疏原始数据 (如果存在，否则为None)\n",
    "        raw_data = sparse_raw_data.get(i, None)\n",
    "        X_raw.append(raw_data)\n",
    "        \n",
    "        # 3. 提取对应的标签 (取序列最后一个点的标签)\n",
    "        label = all_labels[i]\n",
    "        Y_labels.append(label)\n",
    "        \n",
    "    print(f\"Assembled {len(Y_labels)} training samples.\")\n",
    "    return X_sequences, X_raw, Y_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896822a",
   "metadata": {},
   "source": [
    "# 第 5 步：创建 PyTorch Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FidelityDataset(Dataset):\n",
    "    def __init__(self, feature_sequences, raw_data_list, labels):\n",
    "        self.feature_sequences = feature_sequences\n",
    "        self.raw_data_list = raw_data_list\n",
    "        self.labels = labels\n",
    "        print(f\"FidelityDataset created with {len(self.labels)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_seq = self.feature_sequences[idx]\n",
    "        raw_data = self.raw_data_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 将数据转换为Tensor\n",
    "        feature_seq_tensor = torch.tensor(feature_seq, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        if raw_data is not None:\n",
    "            # 如果有原始数据，返回数据本身\n",
    "            raw_data_tensor = torch.tensor(raw_data, dtype=torch.float32)\n",
    "        else:\n",
    "            # 如果没有，返回一个全零的Tensor作为占位符\n",
    "            # DataLoader在打包batch时要求所有样本的形状一致\n",
    "            raw_data_tensor = torch.zeros((200, 11), dtype=torch.float32)\n",
    "            \n",
    "        return feature_seq_tensor, raw_data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d99ba2",
   "metadata": {},
   "source": [
    "## 训练与评估函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b710bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for feature_seq, raw_data, labels in dataloader:\n",
    "        feature_seq = feature_seq.to(device)\n",
    "        raw_data = raw_data.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1) # 形状匹配BCEWithLogitsLoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 注意：我们只关心训练时的logits输出\n",
    "        logits, _, _ = model(feature_seq, raw_data)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * feature_seq.size(0)\n",
    "    \n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for feature_seq, raw_data, labels in dataloader:\n",
    "            feature_seq = feature_seq.to(device)\n",
    "            raw_data = raw_data.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            logits, _, _ = model(feature_seq, raw_data)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * feature_seq.size(0)\n",
    "            \n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032eaed",
   "metadata": {},
   "source": [
    "# 第 6 步：训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 配置参数 ---\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "HISTORY_SEQ_LEN = 60  # 30秒历史\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCHS = 10\n",
    "MODEL_SAVE_PATH = \"fidelity_model_best.pth\"\n",
    "\n",
    "# --- 2. 加载第1步和第2步的成果 ---\n",
    "print(\"Loading pre-processed data...\")\n",
    "all_features = np.load(\"all_features.npy\")\n",
    "all_labels = np.load(\"all_labels.npy\")\n",
    "\n",
    "# --- 3. 运行第三步和第四步 ---\n",
    "X_sequences, X_raw, Y_labels = assemble_training_samples(\n",
    "    all_features, all_labels, sparse_raw_data, history_seq_len=HISTORY_SEQ_LEN\n",
    ")\n",
    "\n",
    "# 划分训练集和验证集 (80/20)\n",
    "# 使用 stratify 确保在类别不平衡时，训练集和验证集有相同的类别比例\n",
    "X_seq_train, X_seq_val, X_raw_train, X_raw_val, y_train, y_val = train_test_split(\n",
    "    X_sequences, X_raw, Y_labels, test_size=0.2, random_state=42, stratify=Y_labels\n",
    ")\n",
    "\n",
    "# 创建Dataset和DataLoader\n",
    "train_dataset = FidelityDataset(X_seq_train, X_raw_train, y_train)\n",
    "val_dataset = FidelityDataset(X_seq_val, X_raw_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 4. 初始化模型并开始训练 ---\n",
    "print(\"\\nInitializing model for training...\")\n",
    "fidelity_model = FidelityModelGated(\n",
    "    feature_dim=6400,\n",
    "    lstm_hidden_dim=256,\n",
    "    raw_cnn_output_dim=6400\n",
    ").to(DEVICE)\n",
    "\n",
    "# 使用BCEWithLogitsLoss，它内置了sigmoid，更稳定\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(fidelity_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_one_epoch(fidelity_model, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_metrics = evaluate(fidelity_model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "        f\"Val Accuracy: {val_metrics['accuracy']:.4f} | \"\n",
    "        f\"Val F1: {val_metrics['f1']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # 保存表现最好的模型（以F1分数为标准）\n",
    "    if val_metrics['f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1']\n",
    "        torch.save(fidelity_model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"  -> New best model saved to {MODEL_SAVE_PATH} (F1: {best_val_f1:.4f})\")\n",
    "\n",
    "print(\"--- Training Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

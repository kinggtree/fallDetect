{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eaad61",
   "metadata": {},
   "source": [
    "# Notice\n",
    "训练之前请确保，特征模型训练后得到的 **`feature_model_1dcnn.pth`** 和 **`scaler_50hz_torch.gz`** 这两个文件存在"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7145f7",
   "metadata": {},
   "source": [
    "# 导入依赖库，定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a246b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "WINDOW_SECONDS = 4\n",
    "WINDOW_SIZE = int(TARGET_SAMPLING_RATE_HZ * WINDOW_SECONDS) # 200 samples for 4 seconds at 50Hz\n",
    "\n",
    "STEP_SECONDS = 1 # 1秒步长\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * STEP_SECONDS)          # 50 samples for 1 second step at 50Hz\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91559b",
   "metadata": {},
   "source": [
    "# 处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcd81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for data in: MobiFall_Dataset\n",
      "\n",
      "Processing and combining 627 unique trials...\n",
      "Successfully processed and combined sensor data for 627 trials.\n",
      "The shape of the final dataset is: X=(9491, 200, 11), y=(9491,)\n"
     ]
    }
   ],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < WINDOW_SIZE: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "def create_sequences(data_list, label_list, seq_length, step):\n",
    "    \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "    # 初始化用于存放最终序列和对应标签的列表\n",
    "    X, y = [], []\n",
    "    # 遍历每一次活动试验的数据\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        trial_label = label_list[i]\n",
    "        # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "        for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "            # 截取一个固定长度（seq_length）的片段作为序列\n",
    "            X.append(trial_data[j:(j + seq_length)])\n",
    "            # 为这个序列分配对应的标签\n",
    "            y.append(trial_label)\n",
    "            \n",
    "    if not X: return np.array([]), np.array([])\n",
    "    # 将列表转换为Numpy数组后返回\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "SensorDataSequences, SensorLabelSequences = create_sequences(trial_arrays, trial_labels, WINDOW_SIZE, STEP)\n",
    "print(f\"The shape of the final dataset is: X={SensorDataSequences.shape}, y={SensorLabelSequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6fe3",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a0d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1):\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 200 -> 100\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: 100 -> 50\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: 50 -> 25\n",
    "        )\n",
    "        \n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        # 输入维度需要计算: 256 (channels) * 25 (length)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 25, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        标准的前向传播，用于训练和评估\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # Conv1d 需要 (N, C, L) 格式, 所以我们需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        # 因为使用 BCEWithLogitsLoss, 所以不需要在这里加 sigmoid\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        仅用于提取中间特征的函数\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11)\n",
    "        \"\"\"\n",
    "        # 同样需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, 200)\n",
    "        \n",
    "        # 只通过特征提取器\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 输出形状将是 (N, 256, 25)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c20b2f",
   "metadata": {},
   "source": [
    "### 生成连续的特征流和标签流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815bae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载现有的特征文件 'all_features.npy' 和标签文件 'all_labels.npy'，且大小符合要求。跳过后续处理。\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"feature_model_1dcnn.pth\"\n",
    "SCALER_PATH = \"scaler_50hz_torch.gz\"\n",
    "\n",
    "# 如果all_features.npy和all_labels.npy已经存在，且all_features.npy大小小于2GB，则直接加载并跳过后续处理\n",
    "if os.path.exists(\"all_features.npy\") and os.path.exists(\"all_labels.npy\") and os.path.getsize(\"all_features.npy\") < 2 * 1024**3:\n",
    "    print(f\"已加载现有的特征文件 'all_features.npy' 和标签文件 'all_labels.npy'，且大小符合要求。跳过后续处理。\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"未找到现有的特征文件，开始生成新的特征文件...\")\n",
    "    # --- 加载模型和标准化器 ---\n",
    "    print(\"正在加载模型和标准化器...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载模型\n",
    "    model = FeatureModel1DCNN(input_channels=11, num_classes=1).to(device)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "        print(f\"模型已从 {MODEL_PATH} 加载\")\n",
    "    else:\n",
    "        print(f\"警告: 在 {MODEL_PATH} 未找到模型文件。将使用随机初始化的模型。\")\n",
    "    model.eval() # 设置为评估模式\n",
    "\n",
    "    # 加载标准化器\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        scaler = joblib.load(SCALER_PATH)\n",
    "        print(f\"标准化器已从 {SCALER_PATH} 加载\")\n",
    "    else:\n",
    "        # 抛出错误并停止执行\n",
    "        raise FileNotFoundError(f\"ERROR: Standard scaler file not found at '{SCALER_PATH}'. Cannot proceed without it.\")\n",
    "\n",
    "\n",
    "    # --- 批量处理数据并提取特征 ---\n",
    "    print(\"\\n开始批量提取特征...\")\n",
    "    all_features_list = []\n",
    "    all_labels_list = []\n",
    "\n",
    "    # `trial_arrays` 和 `trial_labels` 变量是从上一个数据加载单元格中获得的\n",
    "    # 遍历每一次试验的数据\n",
    "    for i, trial_data in enumerate(trial_arrays):\n",
    "        trial_label = trial_labels[i]\n",
    "        \n",
    "        # 在当前试验数据上应用滑动窗口\n",
    "        for j in range(0, len(trial_data) - WINDOW_SIZE + 1, STEP):\n",
    "            # 1. 截取一个窗口的数据\n",
    "            window_data = trial_data[j : j + WINDOW_SIZE]\n",
    "            \n",
    "            # 2. 预处理窗口数据 (标准化 -> 转换为Tensor)\n",
    "            scaled_window = scaler.transform(window_data)\n",
    "            window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 3. 从模型中提取特征\n",
    "            with torch.no_grad(): # 关闭梯度计算以加速\n",
    "                features = model.extract_features(window_tensor)\n",
    "            \n",
    "            # 4. 将特征扁平化并添加到列表中\n",
    "            flattened_features = features.cpu().numpy().flatten()\n",
    "            all_features_list.append(flattened_features)\n",
    "            \n",
    "            # 5. 将该窗口对应的标签添加到列表中\n",
    "            all_labels_list.append(trial_label)\n",
    "\n",
    "    print(f\"处理完成！共处理了 {len(trial_arrays)} 次试验，生成了 {len(all_features_list)} 个特征向量。\")\n",
    "\n",
    "    # --- 4. 保存最终的数据集 ---\n",
    "    if all_features_list:\n",
    "        # 将列表转换为Numpy数组\n",
    "        final_features = np.array(all_features_list)\n",
    "        final_labels = np.array(all_labels_list)\n",
    "\n",
    "        # 保存数组到.npy文件\n",
    "        np.save(\"all_features.npy\", final_features)\n",
    "        np.save(\"all_labels.npy\", final_labels)\n",
    "\n",
    "        print(f\"\\n数据集已成功保存:\")\n",
    "        print(f\"  - 特征文件: all_features.npy, 形状: {final_features.shape}\")\n",
    "        print(f\"  - 标签文件: all_labels.npy, 形状: {final_labels.shape}\")\n",
    "    else:\n",
    "        print(\"\\n未能生成任何特征，未创建文件。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f7f19",
   "metadata": {},
   "source": [
    "### 稀疏化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a680bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_data(data_array, sparsity_ratio):\n",
    "    \"\"\"\n",
    "    Randomly sets a portion of samples in a data array to zero.\n",
    "\n",
    "    Args:\n",
    "        data_array (np.ndarray): The input data array, e.g., shape (9491, 200, 11).\n",
    "        sparsity_ratio (float): The fraction of samples to set to zero (between 0.0 and 1.0).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A new data array with the specified portion of samples zeroed out.\n",
    "    \"\"\"\n",
    "    if not 0.0 <= sparsity_ratio <= 1.0:\n",
    "        raise ValueError(\"Sparsity ratio must be between 0.0 and 1.0\")\n",
    "\n",
    "    # 创建一个副本以避免修改原始数组\n",
    "    sparse_array = data_array.copy()\n",
    "    \n",
    "    # 获取样本总数\n",
    "    num_samples = sparse_array.shape[0]\n",
    "    \n",
    "    # 计算需要置零的样本数量\n",
    "    num_to_zero_out = int(num_samples * sparsity_ratio)\n",
    "    \n",
    "    if num_to_zero_out == 0:\n",
    "        print(\"Sparsity ratio is too low, no samples will be zeroed out.\")\n",
    "        return sparse_array\n",
    "\n",
    "    # 随机选择不重复的索引进行置零\n",
    "    indices_to_zero = np.random.choice(\n",
    "        np.arange(num_samples), \n",
    "        size=num_to_zero_out, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # 将选定索引对应的整个 (200, 11) 向量置零\n",
    "    sparse_array[indices_to_zero] = 0\n",
    "    \n",
    "    print(f\"Sparsification complete:\")\n",
    "    print(f\"  - Total samples: {num_samples}\")\n",
    "    print(f\"  - Sparsity ratio: {sparsity_ratio:.2f}\")\n",
    "    print(f\"  - Samples zeroed out: {len(indices_to_zero)}\")\n",
    "    \n",
    "    return sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fa41c",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7de080",
   "metadata": {},
   "source": [
    "### 时间分布CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde9cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状: (batch_size, time_steps, C, H, W) 或 (batch_size, time_steps, features...)\n",
    "        # 我们这里是 (batch_size, 60, 200, 11)\n",
    "        \n",
    "        batch_size, time_steps = x.size(0), x.size(1)\n",
    "        \n",
    "        # 1. 合并 batch 和 time 维度\n",
    "        # (B, T, C, F) -> (B * T, C, F)\n",
    "        # 我们的输入是 (B, 60, 200, 11)，需要先 permute\n",
    "        x = x.permute(0, 1, 3, 2) # -> (B, 60, 11, 200)\n",
    "        x_reshape = x.contiguous().view(batch_size * time_steps, x.size(2), x.size(3))\n",
    "        # -> (B * 60, 11, 200)\n",
    "\n",
    "        # 2. 应用模块\n",
    "        y = self.module(x_reshape)\n",
    "        \n",
    "        # y 的形状是 (B * 60, output_features)\n",
    "        \n",
    "        # 3. 恢复 batch 和 time 维度\n",
    "        y = y.view(batch_size, time_steps, y.size(-1))\n",
    "        # -> (B, 60, output_features)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa788b",
   "metadata": {},
   "source": [
    "### 交叉注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334715c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query (来自LFS): (Batch, SeqLen, query_dim)\n",
    "        # key/value (来自HFS): (Batch, SeqLen, key_dim)\n",
    "        \n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(key)\n",
    "        V = self.value_layer(value)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 应用权重\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e9d21",
   "metadata": {},
   "source": [
    "### 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8229ac0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_raw_data_cnn():\n",
    "    \"\"\"创建一个用于处理原始传感器数据的1D-CNN模块。\"\"\"\n",
    "    raw_data_processor = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=11, out_channels=64, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(64),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(128),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "        nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "    return raw_data_processor\n",
    "\n",
    "\n",
    "class ContextualFidelityModel(nn.Module):\n",
    "    def __init__(self, feature_dim, lstm_hidden_dim, raw_cnn_output_dim, num_classes=1):\n",
    "        super(ContextualFidelityModel, self).__init__()\n",
    "\n",
    "        # --- 分支一：高保真原始数据流处理器 ---\n",
    "        raw_cnn = create_raw_data_cnn()\n",
    "        self.hfs_processor = TimeDistributed(raw_cnn)\n",
    "\n",
    "        # --- 分支二：低保真特征流处理器 ---\n",
    "        self.lfs_processor = nn.LSTM(\n",
    "            input_size=feature_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "        # --- 融合模块：交叉注意力 ---\n",
    "        # query 来自 lfs_processor (lstm_hidden_dim)\n",
    "        # key/value 来自 hfs_processor (raw_cnn_output_dim)\n",
    "        self.cross_attention = CrossAttention(\n",
    "            query_dim=lstm_hidden_dim,\n",
    "            key_dim=raw_cnn_output_dim,\n",
    "            hidden_dim=lstm_hidden_dim # 通常设置为与query_dim一致\n",
    "        )\n",
    "        \n",
    "        # --- 后融合处理器与分类器 ---\n",
    "        # 将 LSTM 的输出和注意力机制的输出结合起来\n",
    "        self.post_fusion_processor = nn.LSTM(\n",
    "            input_size=lstm_hidden_dim * 2, # Concatenated input\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, imputed_raw_sequence):\n",
    "        # feature_sequence: (B, 60, 6400)\n",
    "        # imputed_raw_sequence: (B, 60, 200, 11)\n",
    "\n",
    "        # 1. 并行处理两条流\n",
    "        lfs_output, _ = self.lfs_processor(feature_sequence) # -> (B, 60, lstm_hidden_dim)\n",
    "        hfs_output = self.hfs_processor(imputed_raw_sequence) # -> (B, 60, raw_cnn_output_dim)\n",
    "\n",
    "        # 2. 交叉注意力融合\n",
    "        # lfs_output 作为 Query，去查询 hfs_output\n",
    "        attention_context = self.cross_attention(\n",
    "            query=lfs_output, \n",
    "            key=hfs_output, \n",
    "            value=hfs_output\n",
    "        ) # -> (B, 60, lstm_hidden_dim)\n",
    "        \n",
    "        # 3. 结合 LFS 输出和注意力上下文\n",
    "        combined_features = torch.cat([lfs_output, attention_context], dim=-1)\n",
    "        # -> (B, 60, lstm_hidden_dim * 2)\n",
    "\n",
    "        # 4. 后融合处理与最终裁决\n",
    "        final_sequence, (h_n, _) = self.post_fusion_processor(combined_features)\n",
    "        \n",
    "        # 使用序列的最后一个时间点的输出进行分类\n",
    "        last_step_output = final_sequence[:, -1, :]\n",
    "        logits = self.classifier(last_step_output)\n",
    "        \n",
    "        # 状态特征依然是最后一个LSTM的隐藏状态\n",
    "        state_feature = h_n.squeeze(0) # -> (B, lstm_hidden_dim)\n",
    "\n",
    "        return logits, state_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896822a",
   "metadata": {},
   "source": [
    "# 创建 PyTorch Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67795c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualFidelityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to create sequences for the ContextualFidelityModel.\n",
    "    Each sample consists of a sequence of features, a sequence of raw data,\n",
    "    and the label corresponding to the last item in the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, raw_data, labels, sequence_length=4):\n",
    "        self.features = features\n",
    "        self.raw_data = raw_data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of possible sequences is the total length minus the sequence length + 1\n",
    "        return len(self.features) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The end index of the sequence slice\n",
    "        end_idx = idx + self.sequence_length\n",
    "\n",
    "        # Get the sequence of features and raw data\n",
    "        feature_seq = self.features[idx:end_idx]\n",
    "        raw_seq = self.raw_data[idx:end_idx]\n",
    "        \n",
    "        # The label corresponds to the final time step in the sequence\n",
    "        label = self.labels[end_idx - 1]\n",
    "\n",
    "        # Convert to tensors\n",
    "        feature_seq_tensor = torch.tensor(feature_seq, dtype=torch.float32)\n",
    "        raw_seq_tensor = torch.tensor(raw_seq, dtype=torch.float32)\n",
    "        # Use float for labels for BCEWithLogitsLoss\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        return feature_seq_tensor, raw_seq_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35be6da",
   "metadata": {},
   "source": [
    "### 加载，分割，创建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Data Preparation ---\")\n",
    "\n",
    "# For demonstration, we'll create dummy files if they don't exist.\n",
    "# In your case, these files should already be generated by your previous script.\n",
    "if not os.path.exists(\"all_features.npy\") or not os.path.exists(\"all_labels.npy\"):\n",
    "    print(\"File 'all_features.npy' or 'all_labels.npy' not found. Exiting.\")\n",
    "    raise FileNotFoundError(\"Required data files are missing.\")\n",
    "\n",
    "\n",
    "final_features = np.load(\"all_features.npy\")\n",
    "final_labels = np.load(\"all_labels.npy\")\n",
    "raw_windows_original = SensorDataSequences \n",
    "\n",
    "# 在这里定义稀疏度，例如 0.3 表示随机将 30% 的数据样本置零\n",
    "SPARSITY_RATIO = 0.2 \n",
    "raw_windows_sparse = create_sparse_data(raw_windows_original, SPARSITY_RATIO)\n",
    "\n",
    "# 验证稀疏化是否成功 (可选)\n",
    "# 一个有效样本的所有值求和后应大于0\n",
    "# non_zero_samples_before = np.count_nonzero(np.sum(raw_windows_original, axis=(1, 2)))\n",
    "# non_zero_samples_after = np.count_nonzero(np.sum(raw_windows_sparse, axis=(1, 2)))\n",
    "# print(f\"  - Non-zero samples before: {non_zero_samples_before}\")\n",
    "# print(f\"  - Non-zero samples after: {non_zero_samples_after}\\n\")\n",
    "\n",
    "\n",
    "# 定义常量\n",
    "SEQUENCE_LENGTH = 4\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.15\n",
    "\n",
    "# 创建 Dataset 实例时，使用稀疏化后的 `raw_windows_sparse`\n",
    "full_dataset = ContextualFidelityDataset(final_features, raw_windows_sparse, final_labels, sequence_length=SEQUENCE_LENGTH)\n",
    "print(f\"Total number of sequences in the dataset: {len(full_dataset)}\")\n",
    "\n",
    "# Create indices for splitting\n",
    "dataset_indices = list(range(len(full_dataset)))\n",
    "train_val_indices, test_indices = train_test_split(dataset_indices, test_size=TEST_SIZE, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=VALIDATION_SIZE / (1 - TEST_SIZE), random_state=42)\n",
    "\n",
    "# Create subsets for train, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032eaed",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Training and Evaluation ---\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "FEATURE_DIM = 6400  # from final_features.shape[1]\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "# Calculate the output dimension of the raw_cnn\n",
    "# Input: (B, 11, 200)\n",
    "# After MaxPool1: 200 / 2 = 100\n",
    "# After MaxPool2: 100 / 2 = 50\n",
    "# After MaxPool3: 50 / 2 = 25\n",
    "# Flattened output: 256 channels * 25 length = 6400\n",
    "RAW_CNN_OUTPUT_DIM = 6400\n",
    "NUM_CLASSES = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 10\n",
    "\n",
    "# Setup device, model, criterion, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ContextualFidelityModel(FEATURE_DIM, LSTM_HIDDEN_DIM, RAW_CNN_OUTPUT_DIM, NUM_CLASSES).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss() # Good for binary classification with one output neuron\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature_seq, raw_seq, labels in loader:\n",
    "            feature_seq, raw_seq, labels = feature_seq.to(device), raw_seq.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(feature_seq, raw_seq)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (feature_seq, raw_seq, labels) in enumerate(train_loader):\n",
    "        feature_seq, raw_seq, labels = feature_seq.to(device), raw_seq.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = model(feature_seq, raw_seq)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# --- 4. Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_prec:.4f}\")\n",
    "print(f\"Test Recall: {test_rec:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

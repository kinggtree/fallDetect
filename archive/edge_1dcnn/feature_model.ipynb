{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b187547",
   "metadata": {},
   "source": [
    "# 数据获取与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cc95d",
   "metadata": {},
   "source": [
    "### 导入库与全局配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438e1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "import joblib\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "WINDOW_SECOND = 2\n",
    "WINDOW_SIZE = int(TARGET_SAMPLING_RATE_HZ * WINDOW_SECOND)  # 200 samples for 4 seconds at 50Hz\n",
    "STEP_SECONDS = 1 # 1秒步长\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * STEP_SECONDS)          # 50 samples for 1 second step at 50Hz\n",
    "\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33811c5f",
   "metadata": {},
   "source": [
    "### 数据加载与预处理函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90cd2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < WINDOW_SIZE: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "def create_sequences(data_list, label_list, seq_length, step):\n",
    "    \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "    # 初始化用于存放最终序列和对应标签的列表\n",
    "    X, y = [], []\n",
    "    # 遍历每一次活动试验的数据\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        trial_label = label_list[i]\n",
    "        # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "        for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "            # 截取一个固定长度（seq_length）的片段作为序列\n",
    "            X.append(trial_data[j:(j + seq_length)])\n",
    "            # 为这个序列分配对应的标签\n",
    "            y.append(trial_label)\n",
    "            \n",
    "    if not X: return np.array([]), np.array([])\n",
    "    # 将列表转换为Numpy数组后返回\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be2f6",
   "metadata": {},
   "source": [
    "### 加载和创建序列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814a72e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for data in: MobiFall_Dataset\n",
      "\n",
      "Processing and combining 627 unique trials...\n",
      "Successfully processed and combined sensor data for 627 trials.\n",
      "The shape of the final dataset is: X=(10745, 100, 11), y=(10745,)\n",
      "Saved processed dataset to npy files.\n"
     ]
    }
   ],
   "source": [
    "SensorDataSequences, SensorLabelSequences = np.array([]), np.array([])\n",
    "\n",
    "if os.path.exists('SensorDataSequences.npy') and os.path.exists('SensorLabelSequences.npy'):\n",
    "    print(\"Found existing npy files. Loading...\")\n",
    "    SensorDataSequences = np.load('SensorDataSequences.npy')\n",
    "    print(f\"Loaded dataset shape: X={SensorDataSequences.shape}\")\n",
    "    SensorLabelSequences = np.load('SensorLabelSequences.npy')\n",
    "    print(f\"Loaded dataset shape: y={SensorLabelSequences.shape}\")\n",
    "else:\n",
    "    trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "    SensorDataSequences, SensorLabelSequences = create_sequences(trial_arrays, trial_labels, WINDOW_SIZE, STEP)\n",
    "    print(f\"The shape of the final dataset is: X={SensorDataSequences.shape}, y={SensorLabelSequences.shape}\")\n",
    "    np.save('SensorDataSequences.npy', SensorDataSequences)\n",
    "    np.save('SensorLabelSequences.npy', SensorLabelSequences)\n",
    "    print(\"Saved processed dataset to npy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576eb140",
   "metadata": {},
   "source": [
    "# 特征模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8173e7c",
   "metadata": {},
   "source": [
    "### 训练数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87a2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (8058, 100, 11); Test set: (2687, 100, 11)\n",
      "Data scaled and scaler saved to scaler_50hz_torch.gz\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Dataset 类\n",
    "\n",
    "class FallDetectionDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# 2. 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    SensorDataSequences, SensorLabelSequences, test_size=0.25, random_state=42, stratify=SensorLabelSequences\n",
    ")\n",
    "print(f\"Train set: {X_train.shape}; Test set: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# 3. 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[2])\n",
    "X_train_scaled_reshaped = scaler.fit_transform(X_train_reshaped)\n",
    "X_train = X_train_scaled_reshaped.reshape(X_train.shape)\n",
    "\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[2])\n",
    "X_test_scaled_reshaped = scaler.transform(X_test_reshaped)\n",
    "X_test = X_test_scaled_reshaped.reshape(X_test.shape)\n",
    "\n",
    "# 保存 scaler\n",
    "scaler_save_path = \"scaler_50hz_torch.gz\"\n",
    "joblib.dump(scaler, scaler_save_path)\n",
    "print(f\"Data scaled and scaler saved to {scaler_save_path}\")\n",
    "\n",
    "\n",
    "# 4. 创建 Dataset 和 DataLoader\n",
    "train_dataset = FallDetectionDataset(X_train, y_train)\n",
    "test_dataset = FallDetectionDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd394d6",
   "metadata": {},
   "source": [
    "### PyTorch 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268f3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1, sequence_length=200): # 添加 sequence_length 参数\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: L -> L/2\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: L/2 -> L/4\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: L/4 -> L/8\n",
    "        )\n",
    "        \n",
    "        # --- 动态计算分类器的输入维度 ---\n",
    "        # 创建一个与真实输入形状相同的虚拟张量\n",
    "        # (batch_size=1, channels=input_channels, length=sequence_length)\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, sequence_length)\n",
    "            # 将虚拟张量传递给特征提取器以获取输出形状\n",
    "            dummy_output = self.feature_extractor(dummy_input)\n",
    "            # 计算展平后的大小\n",
    "            flattened_size = dummy_output.numel() # .numel() 返回张量中元素的总数\n",
    "\n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 512), # <-- 使用动态计算出的大小\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        标准的前向传播，用于训练和评估\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, 200, 11) or (N, 100, 11)\n",
    "        \"\"\"\n",
    "        # Conv1d 需要 (N, C, L) 格式, 所以我们需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, L)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        # 因为我们使用 BCEWithLogitsLoss, 所以不需要在这里加 sigmoid\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        仅用于提取中间特征的函数\n",
    "        x 的输入形状: (batch_size, sequence_length, num_features) -> (N, L, 11)\n",
    "        \"\"\"\n",
    "        # 同样需要重排维度\n",
    "        x = x.permute(0, 2, 1) # -> (N, 11, L)\n",
    "        \n",
    "        # 只通过特征提取器\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # 输出形状将是 (N, 256, L/8)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c65e45",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c0b4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/20, Loss: 0.1310\n",
      "Epoch 2/20, Loss: 0.0752\n",
      "Epoch 3/20, Loss: 0.0563\n",
      "Epoch 4/20, Loss: 0.0731\n",
      "Epoch 5/20, Loss: 0.0478\n",
      "Epoch 6/20, Loss: 0.0425\n",
      "Epoch 7/20, Loss: 0.0416\n",
      "Epoch 8/20, Loss: 0.0418\n",
      "Epoch 9/20, Loss: 0.0384\n",
      "Epoch 10/20, Loss: 0.0364\n",
      "Epoch 11/20, Loss: 0.0309\n",
      "Epoch 12/20, Loss: 0.0247\n",
      "Epoch 13/20, Loss: 0.0264\n",
      "Epoch 14/20, Loss: 0.0438\n",
      "Epoch 15/20, Loss: 0.0293\n",
      "Epoch 16/20, Loss: 0.0413\n",
      "Epoch 17/20, Loss: 0.0317\n",
      "Epoch 18/20, Loss: 0.0280\n",
      "Epoch 19/20, Loss: 0.0180\n",
      "Epoch 20/20, Loss: 0.0204\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 超参数\n",
    "INPUT_CHANNELS = 11\n",
    "NUM_CLASSES = 1 # 二分类\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # 您可以根据需要调整\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "# 修改后的代码\n",
    "model = FeatureModel1DCNN(\n",
    "    input_channels=INPUT_CHANNELS, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    sequence_length=WINDOW_SIZE\n",
    ").to(device)\n",
    "# BCEWithLogitsLoss 自动处理 sigmoid，更稳定\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- 3. 训练循环 ---\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * sequences.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ee79e",
   "metadata": {},
   "source": [
    "### 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca86972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.9929\n",
      "Test Precision: 0.9895\n",
      "Test Recall: 0.9775\n",
      "Test F1-Score: 0.9835\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # 将 logits 转换为概率，再转换为预测类别 (0或1)\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c828dc",
   "metadata": {},
   "source": [
    "### 提取并保存特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0138f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the extracted features array: (2687, 256, 12)\n",
      "Model state saved to feature_model_1dcnn.pth\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_features = []\n",
    "with torch.no_grad():\n",
    "    for sequences, _ in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        \n",
    "        # 使用我们专门定义的方法提取特征\n",
    "        features = model.extract_features(sequences)\n",
    "        \n",
    "        # 将特征移回 CPU 并添加到列表中\n",
    "        all_features.append(features.cpu().numpy())\n",
    "\n",
    "# 将所有批次的特征拼接成一个大的 numpy 数组\n",
    "# 注意：最后一个batch可能不满，所以使用 vstack\n",
    "features_array = np.vstack(all_features)\n",
    "print(f\"Shape of the extracted features array: {features_array.shape}\")\n",
    "\n",
    "# 保存特征到文件\n",
    "# features_save_path = \"extracted_features.npy\"\n",
    "# np.save(features_save_path, features_array)\n",
    "# print(f\"Features saved to {features_save_path}\")\n",
    "\n",
    "# 保存模型权重\n",
    "model_save_path = \"feature_model_1dcnn.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model state saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

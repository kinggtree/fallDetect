{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddacf78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FEATURES_PATH = \"all_features.npy\"\n",
    "LABELS_PATH = \"all_labels.npy\"\n",
    "RAW_DATA_PATH = \"SensorDataSequences.npy\"\n",
    "\n",
    "SCALER_PATH = \"autoregression_timeseries_data_scaler.save\"\n",
    "\n",
    "MODEL_PATH = \"contextual_fidelity_model_pretrained_encoder.pth\"\n",
    "\n",
    "OUTPUT_FEATURES_PATH = \"fidelity_extracted_state_features.npy\"\n",
    "OUTPUT_LABELS_PATH = \"fidelity_extracted_labels.npy\"\n",
    "OUTPUT_PARAM_ROWS_PATH = \"fidelity_extracted_param_rows.npy\"\n",
    "\n",
    "# 稀疏度\n",
    "SPARSITY_RATIO = 0.8\n",
    "SEQUENCE_LENGTH = 4            # 累积x个特征后进行一次推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b029735",
   "metadata": {},
   "source": [
    "### 稀疏化函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44517bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_data(data_array, sparsity_ratio):\n",
    "    print(\"Sparsifying data...\")\n",
    "    sparse_array = data_array.copy()\n",
    "    num_samples = sparse_array.shape[0]\n",
    "    num_to_zero_out = int(num_samples * sparsity_ratio)\n",
    "    indices_to_zero = np.random.choice(np.arange(num_samples), size=num_to_zero_out, replace=False)\n",
    "    sparse_array[indices_to_zero] = 0\n",
    "    print(f\"Sparsification complete. {num_to_zero_out}/{num_samples} samples zeroed.\")\n",
    "    return sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eec9ef",
   "metadata": {},
   "source": [
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce2d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying scaler to raw sensor data sequences...\n",
      "Finished applying scaler to raw sensor data sequences.\n",
      "Sparsifying data...\n",
      "Sparsification complete. 7592/9491 samples zeroed.\n",
      "Sparse raw data sequences shape: (2372, 4, 200, 11)\n",
      "Sparse features_sequences shape: (2372, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "features = np.load(FEATURES_PATH)\n",
    "labels = np.load(LABELS_PATH)\n",
    "\n",
    "# 从文件中加载原始数据\n",
    "raw_data = np.load(RAW_DATA_PATH)\n",
    "\n",
    "# 归一化\n",
    "print(\"\\nApplying scaler to raw sensor data sequences...\")\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "\n",
    "scaled_raw_data = []\n",
    "for i in range(raw_data.shape[0]):\n",
    "    scaled_sequence = scaler.transform(raw_data[i])\n",
    "    scaled_raw_data.append(scaled_sequence)\n",
    "\n",
    "print(\"Finished applying scaler to raw sensor data sequences.\")\n",
    "\n",
    "scaled_raw_data = np.array(scaled_raw_data)\n",
    "\n",
    "# 稀疏化原始数据\n",
    "sparse_raw_data = create_sparse_data(scaled_raw_data, SPARSITY_RATIO)\n",
    "\n",
    "# 按 SEQUENCE_LENGTH 划分序列\n",
    "def create_sequences(data_array, sequence_length):\n",
    "    sequences = []\n",
    "    num_samples = data_array.shape[0]\n",
    "    for start_idx in range(0, num_samples - sequence_length + 1, sequence_length):\n",
    "        end_idx = start_idx + sequence_length\n",
    "        sequence = data_array[start_idx:end_idx]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "sparse_raw_data_sequences = create_sequences(sparse_raw_data, SEQUENCE_LENGTH)\n",
    "features_sequences = create_sequences(features, SEQUENCE_LENGTH)\n",
    "labels_sequences = create_sequences(labels, SEQUENCE_LENGTH)\n",
    "print(f\"Sparse raw data sequences shape: {sparse_raw_data_sequences.shape}\")\n",
    "print(f\"Sparse features_sequences shape: {features_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572321a8",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89db325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributedEncoder(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributedEncoder, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, seq_len, n_features = x.size()\n",
    "\n",
    "        x_reshape = x.contiguous().view(batch_size * time_steps, seq_len, n_features)\n",
    "        hidden, _ = self.module(x_reshape)\n",
    "        output_features = hidden[-1]\n",
    "        y = output_features.view(batch_size, time_steps, -1)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(key)\n",
    "        V = self.value_layer(value)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # 返回最终的 hidden 和 cell 状态\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class ContextualFidelityModel(nn.Module):\n",
    "    def __init__(self, lfs_feature_dim, lstm_hidden_dim, hfs_feature_dim, num_classes=1):\n",
    "        super(ContextualFidelityModel, self).__init__()\n",
    "\n",
    "        hfs_encoder = Encoder(input_dim=11, hidden_dim=hfs_feature_dim, n_layers=2, dropout=0.1)\n",
    "        self.hfs_processor = TimeDistributedEncoder(hfs_encoder)\n",
    "\n",
    "        self.lfs_processor = nn.LSTM(\n",
    "            input_size=lfs_feature_dim, # 将接收64维特征\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            query_dim=lstm_hidden_dim,\n",
    "            key_dim=hfs_feature_dim, # 维度变为64\n",
    "            hidden_dim=lstm_hidden_dim\n",
    "        )\n",
    "        \n",
    "        self.post_fusion_processor = nn.LSTM(\n",
    "            input_size=lstm_hidden_dim * 2, # Concatenated input\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, imputed_raw_sequence):\n",
    "        lfs_output, _ = self.lfs_processor(feature_sequence) # -> (B, 60, lstm_hidden_dim)\n",
    "        hfs_output = self.hfs_processor(imputed_raw_sequence) # -> (B, 60, raw_cnn_output_dim)\n",
    "        attention_context = self.cross_attention(\n",
    "            query=lfs_output, \n",
    "            key=hfs_output, \n",
    "            value=hfs_output\n",
    "        ) # -> (B, 60, lstm_hidden_dim)\n",
    "\n",
    "        combined_features = torch.cat([lfs_output, attention_context], dim=-1)\n",
    "\n",
    "        final_sequence, (h_n, _) = self.post_fusion_processor(combined_features)\n",
    "        \n",
    "        last_step_output = final_sequence[:, -1, :]\n",
    "        logits = self.classifier(last_step_output)\n",
    "        state_feature = h_n.squeeze(0) # -> (B, lstm_hidden_dim)\n",
    "\n",
    "        return logits, state_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b800b",
   "metadata": {},
   "source": [
    "### 模型加载函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b516c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 模型加载 ---\n",
    "def load_model(model_path):\n",
    "    \"\"\"加载训练好的模型并设置为评估模式\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    # --- 模型超参数 (需要和训练时保持一致) ---\n",
    "    LFS_FEATURE_DIM = 64\n",
    "    HFS_FEATURE_DIM = 64 \n",
    "    LSTM_HIDDEN_DIM = 256\n",
    "    NUM_CLASSES = 1\n",
    "    \n",
    "    model = ContextualFidelityModel(\n",
    "        lfs_feature_dim=LFS_FEATURE_DIM, \n",
    "        lstm_hidden_dim=LSTM_HIDDEN_DIM, \n",
    "        hfs_feature_dim=HFS_FEATURE_DIM, \n",
    "        num_classes=NUM_CLASSES\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Model file not found at {model_path}. Please train and save the model first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "    model.eval() # 设置为评估模式\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b852a",
   "metadata": {},
   "source": [
    "### 主运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d233e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from contextual_fidelity_model_pretrained_encoder.pth...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "feature_sequence = []\n",
    "label_sequence = []\n",
    "\n",
    "total_predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "save_counter = 0\n",
    "\n",
    "list_index = 0\n",
    "\n",
    "# 创建一个保存参数的列表\n",
    "param_rows = []\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "all_state_features = []\n",
    "\n",
    "while list_index < len(features_sequences):\n",
    "    # 1. 累积一个序列长度的特征\n",
    "    feature_sequence = features_sequences[list_index]\n",
    "    label_sequence = labels_sequences[list_index]\n",
    "\n",
    "    raw_data_array = sparse_raw_data_sequences[list_index]\n",
    "    zero_vectors_count = np.sum(np.all(raw_data_array == 0, axis=(1, 2)))\n",
    "\n",
    "    list_index += 1\n",
    "\n",
    "    # 3. 准备模型输入\n",
    "    feature_tensor = torch.tensor(np.array(feature_sequence), dtype=torch.float32).unsqueeze(0) # (1, 4, 6400)\n",
    "    raw_data_tensor = torch.tensor(raw_data_array, dtype=torch.float32).unsqueeze(0) # (1, 4, 200, 11)\n",
    "    \n",
    "    # 4. 模型推理\n",
    "    with torch.no_grad():\n",
    "        logits, state_feature = model(feature_tensor, raw_data_tensor)\n",
    "\n",
    "    state_feature_np = state_feature.detach().cpu().numpy().squeeze(0)\n",
    "    all_state_features.append(state_feature_np)\n",
    "        \n",
    "    prediction_prob = torch.sigmoid(logits).item()\n",
    "    prediction = 1 if prediction_prob > 0.5 else 0\n",
    "    \n",
    "    # 5. 判断结果\n",
    "    # 标签对应序列中的最后一个\n",
    "    true_label = label_sequence[-1]\n",
    "    \n",
    "    total_predictions += 1\n",
    "    is_correct = (prediction == true_label)\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    # 保存参数到DataFrame\n",
    "    param_rows.append({\n",
    "        \"Zero_Vectors_Ratio\": zero_vectors_count / raw_data_array.shape[0],\n",
    "        \"Probability\": prediction_prob,\n",
    "        \"Predict_Label\": prediction,\n",
    "        \"True_Label\": true_label,\n",
    "        \"Result\": 1 if is_correct else 0\n",
    "    })\n",
    "\n",
    "    # 6. 清空列表，为下一个序列做准备\n",
    "    feature_sequence = []\n",
    "    label_sequence = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb1040",
   "metadata": {},
   "source": [
    "### 保存状态特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3355e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete. Saving state features...\n",
      "Final array shape: (2372, 256)\n",
      "Sparse raw data sequences shape: (2372, 4, 200, 11)\n",
      "Labels shape: (2372, 4)\n"
     ]
    }
   ],
   "source": [
    "# 将所有收集到的状态特征保存到单个文件中\n",
    "print(f\"\\nInference complete. Saving state features...\")\n",
    "np.save(OUTPUT_FEATURES_PATH, np.array(all_state_features))\n",
    "np.save(OUTPUT_LABELS_PATH, labels_sequences)\n",
    "print(f\"Final array shape: {np.array(all_state_features).shape}\")\n",
    "print(f\"Sparse raw data sequences shape: {sparse_raw_data_sequences.shape}\")\n",
    "print(f\"Labels shape: {labels_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f3c672",
   "metadata": {},
   "source": [
    "### 保存预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d9c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of param_rows DataFrame: (2372, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Zero_Vectors_Ratio': np.float64(1.0),\n",
       " 'Probability': 0.006587708368897438,\n",
       " 'Predict_Label': 0,\n",
       " 'True_Label': np.int64(1),\n",
       " 'Result': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of param_rows DataFrame: {pd.DataFrame(param_rows).shape}\")\n",
    "np.save(OUTPUT_PARAM_ROWS_PATH, np.array(param_rows))\n",
    "param_rows[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

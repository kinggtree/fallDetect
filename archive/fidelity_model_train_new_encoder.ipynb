{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eaad61",
   "metadata": {},
   "source": [
    "# Notice\n",
    "训练之前请确保，特征模型训练后得到的 **`feature_model_1dcnn.pth`** 和 **`scaler_50hz_torch.gz`** 这两个文件存在，且训练时的窗口大小和序列长度与本笔记本中的参数保持一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7145f7",
   "metadata": {},
   "source": [
    "# 导入依赖库，定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a246b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "# 生成原始数据的参数\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "WINDOW_SECONDS = 4\n",
    "WINDOW_SIZE = int(TARGET_SAMPLING_RATE_HZ * WINDOW_SECONDS) # x samples for y seconds at 50Hz\n",
    "STEP_SECONDS = 1 # 1秒步长\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * STEP_SECONDS)          # 50 samples for 1 second step at 50Hz\n",
    "\n",
    "# 生成上下文数据集的参数\n",
    "SEQUENCE_LENGTH = 4  # 每个序列包含 x 个 WINDOW_SIZE\n",
    "STRIDE = 2           # 每隔 x 个 WINDOW_SIZE 创建一个新的序列\n",
    "\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91559b",
   "metadata": {},
   "source": [
    "# 处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfcd81a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for data in: MobiFall_Dataset\n",
      "\n",
      "Processing and combining 627 unique trials...\n",
      "Successfully processed and combined sensor data for 627 trials.\n",
      "Found existing npy files. Loading...\n",
      "Loaded dataset shape: X=(9491, 200, 11)\n",
      "Loaded dataset shape: y=(9491,)\n"
     ]
    }
   ],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < WINDOW_SIZE: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "def create_sequences(data_list, label_list, seq_length, step):\n",
    "    \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "    # 初始化用于存放最终序列和对应标签的列表\n",
    "    X, y = [], []\n",
    "    # 遍历每一次活动试验的数据\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        trial_label = label_list[i]\n",
    "        # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "        for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "            # 截取一个固定长度（seq_length）的片段作为序列\n",
    "            X.append(trial_data[j:(j + seq_length)])\n",
    "            # 为这个序列分配对应的标签\n",
    "            y.append(trial_label)\n",
    "            \n",
    "    if not X: return np.array([]), np.array([])\n",
    "    # 将列表转换为Numpy数组后返回\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SensorDataSequences, SensorLabelSequences = np.array([]), np.array([])\n",
    "trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "\n",
    "if os.path.exists('SensorDataSequences.npy') and os.path.exists('SensorLabelSequences.npy'):\n",
    "    print(\"Found existing npy files. Loading...\")\n",
    "    SensorDataSequences = np.load('SensorDataSequences.npy')\n",
    "    print(f\"Loaded dataset shape: X={SensorDataSequences.shape}\")\n",
    "    SensorLabelSequences = np.load('SensorLabelSequences.npy')\n",
    "    print(f\"Loaded dataset shape: y={SensorLabelSequences.shape}\")\n",
    "else:\n",
    "    SensorDataSequences, SensorLabelSequences = create_sequences(trial_arrays, trial_labels, WINDOW_SIZE, STEP)\n",
    "    print(f\"The shape of the final dataset is: X={SensorDataSequences.shape}, y={SensorLabelSequences.shape}\")\n",
    "    np.save('SensorDataSequences.npy', SensorDataSequences)\n",
    "    np.save('SensorLabelSequences.npy', SensorLabelSequences)\n",
    "    print(\"Saved processed dataset to npy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f6fe3",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a0d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureModel1DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=11, num_classes=1, sequence_length=200): # 添加 sequence_length 参数\n",
    "        super(FeatureModel1DCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取器: 包含一系列的卷积和池化层\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: L -> L/2\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2), # Length: L/2 -> L/4\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)  # Length: L/4 -> L/8\n",
    "        )\n",
    "        \n",
    "        # --- 动态计算分类器的输入维度 ---\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, sequence_length)\n",
    "            dummy_output = self.feature_extractor(dummy_input)\n",
    "            flattened_size = dummy_output.numel()\n",
    "\n",
    "        # 分类器: 将提取的特征映射到最终的输出\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, 512), # <-- 使用动态计算出的大小\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        features = self.feature_extractor(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c20b2f",
   "metadata": {},
   "source": [
    "### 生成连续的特征流和标签流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815bae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载现有的特征文件 'all_features.npy' 和标签文件 'all_labels.npy'，且大小符合要求。跳过后续处理。\n"
     ]
    }
   ],
   "source": [
    "# 如果all_features.npy和all_labels.npy已经存在，且all_features.npy大小小于2GB，则直接加载并跳过后续处理\n",
    "if os.path.exists(\"all_features.npy\") and os.path.exists(\"all_labels.npy\") and os.path.getsize(\"all_features.npy\") < 2 * 1024**3:\n",
    "    print(f\"已加载现有的特征文件 'all_features.npy' 和标签文件 'all_labels.npy'，且大小符合要求。跳过后续处理。\")\n",
    "else:\n",
    "    raise RuntimeError(\"请先运行 autoregression_feature_extract.ipynb 生成特征及其标签，并检查特征维度，然后再运行此脚本。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f7f19",
   "metadata": {},
   "source": [
    "### 稀疏化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a680bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_data(data_array, sparsity_ratio):\n",
    "    \"\"\"\n",
    "    Randomly sets a portion of samples in a data array to zero.\n",
    "\n",
    "    Args:\n",
    "        data_array (np.ndarray): The input data array, e.g., shape (9491, 200, 11).\n",
    "        sparsity_ratio (float): The fraction of samples to set to zero (between 0.0 and 1.0).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A new data array with the specified portion of samples zeroed out.\n",
    "    \"\"\"\n",
    "    if not 0.0 <= sparsity_ratio <= 1.0:\n",
    "        raise ValueError(\"Sparsity ratio must be between 0.0 and 1.0\")\n",
    "\n",
    "    # 创建一个副本以避免修改原始数组\n",
    "    sparse_array = data_array.copy()\n",
    "    \n",
    "    # 获取样本总数\n",
    "    num_samples = sparse_array.shape[0]\n",
    "    \n",
    "    # 计算需要置零的样本数量\n",
    "    num_to_zero_out = int(num_samples * sparsity_ratio)\n",
    "    \n",
    "    if num_to_zero_out == 0:\n",
    "        print(\"Sparsity ratio is too low, no samples will be zeroed out.\")\n",
    "        return sparse_array\n",
    "\n",
    "    # 随机选择不重复的索引进行置零\n",
    "    indices_to_zero = np.random.choice(\n",
    "        np.arange(num_samples), \n",
    "        size=num_to_zero_out, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    # 将选定索引对应的整个 (200, 11) 向量置零\n",
    "    sparse_array[indices_to_zero] = 0\n",
    "    \n",
    "    print(f\"Sparsification complete:\")\n",
    "    print(f\"  - Total samples: {num_samples}\")\n",
    "    print(f\"  - Sparsity ratio: {sparsity_ratio:.2f}\")\n",
    "    print(f\"  - Samples zeroed out: {len(indices_to_zero)}\")\n",
    "    \n",
    "    return sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7fa41c",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7de080",
   "metadata": {},
   "source": [
    "### 时间分布解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde9cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributedEncoder(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributedEncoder, self).__init__()\n",
    "        self.module = module  # module 将是一个Encoder实例\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, time_steps, window_size, num_features) e.g., (32, 8, 200, 11)\n",
    "        batch_size, time_steps, seq_len, n_features = x.size()\n",
    "\n",
    "        # 将 batch 和 time_steps 维度合并，以适配Encoder的输入\n",
    "        x_reshape = x.contiguous().view(batch_size * time_steps, seq_len, n_features)\n",
    "        \n",
    "        # 通过Encoder，得到hidden state\n",
    "        hidden, _ = self.module(x_reshape)\n",
    "        # hidden shape: (num_layers, batch_size * time_steps, hidden_dim)\n",
    "\n",
    "        # 我们只使用最后一层的hidden state作为该时间步的特征\n",
    "        output_features = hidden[-1]\n",
    "        # output_features shape: (batch_size * time_steps, hidden_dim)\n",
    "\n",
    "        # 将维度恢复成 (batch_size, time_steps, feature_dim)\n",
    "        y = output_features.view(batch_size, time_steps, -1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa788b",
   "metadata": {},
   "source": [
    "### 交叉注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334715c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, hidden_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(key_dim, hidden_dim)\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # query (来自LFS): (Batch, SeqLen, query_dim)\n",
    "        # key/value (来自HFS): (Batch, SeqLen, key_dim)\n",
    "        \n",
    "        Q = self.query_layer(query)\n",
    "        K = self.key_layer(key)\n",
    "        V = self.value_layer(value)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 应用权重\n",
    "        context_vector = torch.matmul(attention_weights, V)\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86543617",
   "metadata": {},
   "source": [
    "### 自回归Encoder模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e7b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # 返回最终的 hidden 和 cell 状态\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e9d21",
   "metadata": {},
   "source": [
    "### 最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8229ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualFidelityModel(nn.Module):\n",
    "    # 参数名修改得更清晰\n",
    "    def __init__(self, lfs_feature_dim, lstm_hidden_dim, hfs_feature_dim, num_classes=1):\n",
    "        super(ContextualFidelityModel, self).__init__()\n",
    "\n",
    "        # --- HFS 处理器: 使用新的Encoder模型 ---\n",
    "        # 注意：这里的参数(如hidden_dim)需要与你训练Encoder时一致\n",
    "        hfs_encoder = Encoder(input_dim=11, hidden_dim=hfs_feature_dim, n_layers=2, dropout=0.1)\n",
    "        self.hfs_processor = TimeDistributedEncoder(hfs_encoder)\n",
    "\n",
    "        # --- LFS 处理器: input_size由外部传入 ---\n",
    "        self.lfs_processor = nn.LSTM(\n",
    "            input_size=lfs_feature_dim, # 将接收64维特征\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "\n",
    "        # --- 交叉注意力: key_dim现在是HFS Encoder的输出维度 ---\n",
    "        self.cross_attention = CrossAttention(\n",
    "            query_dim=lstm_hidden_dim,\n",
    "            key_dim=hfs_feature_dim, # 维度变为64\n",
    "            hidden_dim=lstm_hidden_dim\n",
    "        )\n",
    "        \n",
    "        # --- 后融合处理器与分类器 ---\n",
    "        # 将 LSTM 的输出和注意力机制的输出结合起来\n",
    "        self.post_fusion_processor = nn.LSTM(\n",
    "            input_size=lstm_hidden_dim * 2, # Concatenated input\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, feature_sequence, imputed_raw_sequence):\n",
    "        # feature_sequence: (B, 60, 6400)\n",
    "        # imputed_raw_sequence: (B, 60, 200, 11)\n",
    "\n",
    "        # 1. 并行处理两条流\n",
    "        lfs_output, _ = self.lfs_processor(feature_sequence) # -> (B, 60, lstm_hidden_dim)\n",
    "        hfs_output = self.hfs_processor(imputed_raw_sequence) # -> (B, 60, raw_cnn_output_dim)\n",
    "\n",
    "        # 2. 交叉注意力融合\n",
    "        # lfs_output 作为 Query，去查询 hfs_output\n",
    "        attention_context = self.cross_attention(\n",
    "            query=lfs_output, \n",
    "            key=hfs_output, \n",
    "            value=hfs_output\n",
    "        ) # -> (B, 60, lstm_hidden_dim)\n",
    "        \n",
    "        # 3. 结合 LFS 输出和注意力上下文\n",
    "        combined_features = torch.cat([lfs_output, attention_context], dim=-1)\n",
    "        # -> (B, 60, lstm_hidden_dim * 2)\n",
    "\n",
    "        # 4. 后融合处理与最终裁决\n",
    "        final_sequence, (h_n, _) = self.post_fusion_processor(combined_features)\n",
    "        \n",
    "        # 使用序列的最后一个时间点的输出进行分类\n",
    "        last_step_output = final_sequence[:, -1, :]\n",
    "        logits = self.classifier(last_step_output)\n",
    "        \n",
    "        # 状态特征依然是最后一个LSTM的隐藏状态\n",
    "        state_feature = h_n.squeeze(0) # -> (B, lstm_hidden_dim)\n",
    "\n",
    "        return logits, state_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896822a",
   "metadata": {},
   "source": [
    "# 创建 PyTorch Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67795c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualFidelityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to create sequences for the ContextualFidelityModel.\n",
    "    \n",
    "    Args:\n",
    "        features (np.ndarray): The array of pre-extracted features.\n",
    "        raw_data (np.ndarray): The array of raw sensor data windows.\n",
    "        labels (np.ndarray): The array of labels.\n",
    "        sequence_length (int): The number of time steps in each sequence (e.g., 4).\n",
    "        stride (int): The step size to slide the window across the data. \n",
    "                      A stride of 1 creates maximum overlap, while a stride equal \n",
    "                      to sequence_length creates no overlap.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, raw_data, labels, sequence_length=4, stride=1):\n",
    "        self.features = features\n",
    "        self.raw_data = raw_data\n",
    "        self.labels = labels\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # 计算并存储数据集的有效长度\n",
    "        self.num_sequences = (len(self.features) - self.sequence_length) // self.stride + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回预先计算好的序列总数\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 根据序列索引(idx)和步长(stride)计算在原始数据中的起始位置\n",
    "        start_idx = idx * self.stride\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        # 2. 切片特征和原始数据\n",
    "        feature_seq = self.features[start_idx:end_idx]\n",
    "        raw_seq = self.raw_data[start_idx:end_idx]\n",
    "        \n",
    "        # 3. 应用新的标签生成规则\n",
    "        #   - 切片这个序列对应的所有标签\n",
    "        label_slice = self.labels[start_idx:end_idx]\n",
    "        #   - 如果这个切片中任何一个标签为1，则最终标签为1，否则为0。\n",
    "        #     这等同于取这个切片中的最大值。\n",
    "        label = np.max(label_slice)\n",
    "\n",
    "        # 转换为Tensors\n",
    "        feature_seq_tensor = torch.tensor(feature_seq, dtype=torch.float32)\n",
    "        raw_seq_tensor = torch.tensor(raw_seq, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        return feature_seq_tensor, raw_seq_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35be6da",
   "metadata": {},
   "source": [
    "### 加载，分割，创建DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db84ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Preparation ---\n",
      "Dataset Hyperparameters:\n",
      "  - Sequence Length (time_steps): 4\n",
      "  - Stride: 2\n",
      "\n",
      "Dataset size changed:\n",
      "  - Original size (stride=1): 9488 sequences\n",
      "  - New augmented size (stride=2): 4744 sequences\n",
      "  - Augmentation factor: 0.50x\n",
      "Total number of sequences in the dataset: 9488\n",
      "Training samples: 6166\n",
      "Validation samples: 1424\n",
      "Test samples: 1898\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Starting Data Preparation ---\")\n",
    "\n",
    "# 定义常量\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.2\n",
    "VALIDATION_SIZE = 0.15\n",
    "\n",
    "if not os.path.exists(\"all_features.npy\") or not os.path.exists(\"all_labels.npy\"):\n",
    "    print(\"File 'all_features.npy' or 'all_labels.npy' not found. Exiting.\")\n",
    "    raise FileNotFoundError(\"Required data files are missing.\")\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "final_features = np.load(\"all_features.npy\")\n",
    "final_labels = np.load(\"all_labels.npy\")\n",
    "raw_windows_original = SensorDataSequences\n",
    "\n",
    "# 数据稀疏化\n",
    "# SPARSITY_RATIO = 0.1\n",
    "# raw_windows = create_sparse_data(raw_windows_original, SPARSITY_RATIO)\n",
    "# 不使用稀疏化\n",
    "raw_windows = raw_windows_original\n",
    "\n",
    "\n",
    "print(f\"Dataset Hyperparameters:\")\n",
    "print(f\"  - Sequence Length (time_steps): {SEQUENCE_LENGTH}\")\n",
    "print(f\"  - Stride: {STRIDE}\")\n",
    "\n",
    "# 创建 Dataset 实例时，传入 sequence_length 和 stride\n",
    "full_dataset = ContextualFidelityDataset(\n",
    "    final_features, \n",
    "    raw_windows, \n",
    "    final_labels, \n",
    "    sequence_length=SEQUENCE_LENGTH, \n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "# 打印数据集大小变化以作验证\n",
    "original_len = len(final_features) - SEQUENCE_LENGTH + 1\n",
    "new_len = len(full_dataset)\n",
    "print(f\"\\nDataset size changed:\")\n",
    "print(f\"  - Original size (stride=1): {original_len} sequences\")\n",
    "print(f\"  - New augmented size (stride={STRIDE}): {new_len} sequences\")\n",
    "print(f\"  - Augmentation factor: {new_len / original_len:.2f}x\")\n",
    "\n",
    "# 创建 Dataset 实例时，使用稀疏化后的 `raw_windows_sparse`\n",
    "full_dataset = ContextualFidelityDataset(final_features, raw_windows, final_labels, sequence_length=SEQUENCE_LENGTH)\n",
    "print(f\"Total number of sequences in the dataset: {len(full_dataset)}\")\n",
    "\n",
    "# Create indices for splitting\n",
    "dataset_indices = list(range(len(full_dataset)))\n",
    "train_val_indices, test_indices = train_test_split(dataset_indices, test_size=TEST_SIZE, random_state=42)\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=VALIDATION_SIZE / (1 - TEST_SIZE), random_state=42)\n",
    "\n",
    "# Create subsets for train, validation, and test\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032eaed",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24e08d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Training and Evaluation ---\n",
      "\n",
      "Model Hyperparameters:\n",
      "  - LFS Feature Dimension: 64\n",
      "  - HFS Feature Dimension: 64\n",
      "  - LSTM Hidden Dimension: 256\n",
      "Epoch [1/15] | Train Loss: 0.4143 | Val Loss: 0.2793 | Val Acc: 0.9298 | Val F1: 0.7992\n",
      "Epoch [2/15] | Train Loss: 0.1833 | Val Loss: 0.1454 | Val Acc: 0.9572 | Val F1: 0.8868\n",
      "Epoch [3/15] | Train Loss: 0.1309 | Val Loss: 0.1295 | Val Acc: 0.9586 | Val F1: 0.8917\n",
      "Epoch [4/15] | Train Loss: 0.1078 | Val Loss: 0.1222 | Val Acc: 0.9621 | Val F1: 0.9018\n",
      "Epoch [5/15] | Train Loss: 0.1003 | Val Loss: 0.1203 | Val Acc: 0.9607 | Val F1: 0.8939\n",
      "Epoch [6/15] | Train Loss: 0.0929 | Val Loss: 0.1003 | Val Acc: 0.9649 | Val F1: 0.9084\n",
      "Epoch [7/15] | Train Loss: 0.0824 | Val Loss: 0.1014 | Val Acc: 0.9698 | Val F1: 0.9205\n",
      "Epoch [8/15] | Train Loss: 0.0732 | Val Loss: 0.0811 | Val Acc: 0.9712 | Val F1: 0.9236\n",
      "Epoch [9/15] | Train Loss: 0.0677 | Val Loss: 0.0801 | Val Acc: 0.9698 | Val F1: 0.9199\n",
      "Epoch [10/15] | Train Loss: 0.0633 | Val Loss: 0.0718 | Val Acc: 0.9761 | Val F1: 0.9366\n",
      "Epoch [11/15] | Train Loss: 0.0548 | Val Loss: 0.0762 | Val Acc: 0.9747 | Val F1: 0.9373\n",
      "Epoch [12/15] | Train Loss: 0.0532 | Val Loss: 0.0548 | Val Acc: 0.9789 | Val F1: 0.9460\n",
      "Epoch [13/15] | Train Loss: 0.0413 | Val Loss: 0.0499 | Val Acc: 0.9824 | Val F1: 0.9533\n",
      "Epoch [14/15] | Train Loss: 0.0420 | Val Loss: 0.0503 | Val Acc: 0.9831 | Val F1: 0.9574\n",
      "Epoch [15/15] | Train Loss: 0.0330 | Val Loss: 0.0391 | Val Acc: 0.9838 | Val F1: 0.9583\n",
      "\n",
      "Training finished.\n",
      "\n",
      "--- Evaluating on Test Set ---\n",
      "Test Loss: 0.0358\n",
      "Test Accuracy: 0.9858\n",
      "Test Precision: 0.9437\n",
      "Test Recall: 0.9832\n",
      "Test F1-Score: 0.9631\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Training and Evaluation ---\")\n",
    "\n",
    "LFS_FEATURE_DIM = final_features.shape[1] \n",
    "\n",
    "# HFS特征维度现在由Encoder的hidden_dim决定\n",
    "HFS_FEATURE_DIM = 64 \n",
    "\n",
    "if HFS_FEATURE_DIM != LFS_FEATURE_DIM:\n",
    "    raise RuntimeError(f\"HFS feature dimension ({HFS_FEATURE_DIM}) does not match LFS feature dimension ({LFS_FEATURE_DIM}). Please check the Encoder configuration.\")\n",
    "\n",
    "LSTM_HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"\\nModel Hyperparameters:\")\n",
    "print(f\"  - LFS Feature Dimension: {LFS_FEATURE_DIM}\")\n",
    "print(f\"  - HFS Feature Dimension: {HFS_FEATURE_DIM}\")\n",
    "print(f\"  - LSTM Hidden Dimension: {LSTM_HIDDEN_DIM}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 使用新的维度参数来实例化模型\n",
    "model = ContextualFidelityModel(\n",
    "    lfs_feature_dim=LFS_FEATURE_DIM, \n",
    "    lstm_hidden_dim=LSTM_HIDDEN_DIM, \n",
    "    hfs_feature_dim=HFS_FEATURE_DIM, \n",
    "    num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature_seq, raw_seq, labels in loader:\n",
    "            feature_seq, raw_seq, labels = feature_seq.to(device), raw_seq.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(feature_seq, raw_seq)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (feature_seq, raw_seq, labels) in enumerate(train_loader):\n",
    "        feature_seq, raw_seq, labels = feature_seq.to(device), raw_seq.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = model(feature_seq, raw_seq)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'contextual_fidelity_model_new_encoder.pth')\n",
    "\n",
    "# --- 4. Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_prec:.4f}\")\n",
    "print(f\"Test Recall: {test_rec:.4f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

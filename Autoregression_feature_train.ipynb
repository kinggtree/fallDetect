{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a85fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "DATASET_PATH = 'MobiFall_Dataset'\n",
    "TARGET_SAMPLING_RATE_HZ = 50.0  # Target sampling rate in Hz\n",
    "TARGET_SAMPLING_PERIOD = f\"{int(1000 / TARGET_SAMPLING_RATE_HZ)}ms\"\n",
    "WINDOW_SECONDS = 4\n",
    "WINDOW_SIZE = int(TARGET_SAMPLING_RATE_HZ * WINDOW_SECONDS) # x samples for y seconds at 50Hz\n",
    "\n",
    "STEP_SECONDS = 1 # x秒步长\n",
    "STEP = int(TARGET_SAMPLING_RATE_HZ * STEP_SECONDS)          # 50*x samples for x second step at 50Hz\n",
    "\n",
    "SENSOR_CODES = [\"acc\", \"gyro\", \"ori\"]\n",
    "EXPECTED_COLUMNS = {\n",
    "    \"acc\": [\"acc_x\", \"acc_y\", \"acc_z\"],\n",
    "    \"gyro\": [\"gyro_x\", \"gyro_y\", \"gyro_z\"],\n",
    "    \"ori\": [\"ori_azimuth\", \"ori_pitch\", \"ori_roll\"]\n",
    "}\n",
    "ALL_FEATURE_COLUMNS = [\n",
    "    \"acc_x\", \"acc_y\", \"acc_z\", \"acc_smv\",\n",
    "    \"gyro_x\", \"gyro_y\", \"gyro_z\", \"gyro_smv\",\n",
    "    \"ori_azimuth\", \"ori_pitch\", \"ori_roll\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db374",
   "metadata": {},
   "source": [
    "## 处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d202867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for data in: MobiFall_Dataset\n",
      "\n",
      "Processing and combining 627 unique trials...\n",
      "Successfully processed and combined sensor data for 627 trials.\n",
      "Found existing npy files. Loading...\n",
      "Loaded dataset shape: X=(9491, 200, 11)\n",
      "Loaded dataset shape: y=(9491,)\n"
     ]
    }
   ],
   "source": [
    "def load_and_resample_sensor_file(filepath, sensor_code):\n",
    "    \"\"\"加载单个传感器文件，转换时间戳并进行重采样。\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # 初始化一个变量作为“标记未找到”的标志\n",
    "        data_start_line_index = -1\n",
    "\n",
    "        # 遍历文件中的每一行\n",
    "        for i, line in enumerate(lines):\n",
    "            # 检查当前行是否是\"@DATA\"标记\n",
    "            if line.strip().upper() == \"@DATA\":\n",
    "                # 如果是，则记录下一行的行号并跳出循环\n",
    "                data_start_line_index = i + 1\n",
    "                break\n",
    "\n",
    "        # 检查标记是否被找到\n",
    "        if data_start_line_index == -1 or data_start_line_index >= len(lines):\n",
    "            return None\n",
    "\n",
    "        # 将数据行拼接成单个字符串\n",
    "        data_string = \"\".join(lines[data_start_line_index:])\n",
    "\n",
    "        # 检查字符串是否为空\n",
    "        if not data_string.strip():\n",
    "            return None\n",
    "\n",
    "        # 使用pandas处理数据\n",
    "        df = pd.read_csv(io.StringIO(data_string), header=None, usecols=[0, 1, 2, 3])\n",
    "        \n",
    "        # 检查生成的数据表是否为空\n",
    "        if df.empty:\n",
    "            return None\n",
    "\n",
    "        # 为数据列进行命名\n",
    "        df.columns = ['timestamp_ns'] + EXPECTED_COLUMNS[sensor_code]\n",
    "\n",
    "        # 将ns时间戳转换为标准的日期时间格式\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp_ns'], unit='ns')\n",
    "\n",
    "        # 将新的日期时间设置为索引，并删除旧的时间戳列\n",
    "        df = df.set_index('timestamp').drop(columns=['timestamp_ns'])\n",
    "\n",
    "        # 按时间索引进行排序\n",
    "        df = df.sort_index()\n",
    "\n",
    "        # 将采样时间不均匀的传感器数据，强制转换为频率统一（每20毫秒一个点）的规整数据流，并填补其中的所有空白\n",
    "        df_resampled = df.resample(TARGET_SAMPLING_PERIOD).mean().interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "        # 检查当前处理的传感器是否为加速度计 ('acc')\n",
    "        if sensor_code == 'acc':\n",
    "            # 安全性检查 - 确认三轴数据都存在\n",
    "            if all(col in df_resampled.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "                # 计算信号幅值向量 (SMV)\n",
    "                df_resampled['acc_smv'] = np.sqrt(\n",
    "                    df_resampled['acc_x']**2 + df_resampled['acc_y']**2 + df_resampled['acc_z']**2\n",
    "                )\n",
    "\n",
    "        # 如果不是加速度计，则检查是否为陀螺仪 ('gyro')\n",
    "        elif sensor_code == 'gyro':\n",
    "            # 对陀螺仪数据执行相同的操作\n",
    "            if all(col in df_resampled.columns for col in ['gyro_x', 'gyro_y', 'gyro_z']):\n",
    "                df_resampled['gyro_smv'] = np.sqrt(\n",
    "                    df_resampled['gyro_x']**2 + df_resampled['gyro_y']**2 + df_resampled['gyro_z']**2\n",
    "                )\n",
    "\n",
    "        return df_resampled\n",
    "\n",
    "    except (pd.errors.EmptyDataError, ValueError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_structured_folders(dataset_root_path):\n",
    "    \"\"\"遍历数据集文件夹，处理、对齐并组合每个试验的传感器数据。\"\"\"\n",
    "    print(f\"Scanning for data in: {dataset_root_path}\")\n",
    "    if not os.path.isdir(dataset_root_path):\n",
    "        print(f\"ERROR: Dataset root path '{dataset_root_path}' not found.\")\n",
    "        return [], []\n",
    "\n",
    "    # 存放每一次活动试验（trial）所对应的各个传感器文件的路径（数据文件的位置）\n",
    "    trial_sensor_files_map = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "    # 存放每一次活动试验的元数据（这些数据代表什么，即标签信息）\n",
    "    trial_metadata_map = {}\n",
    "    \n",
    "    # 遍历数据集的每一个文件夹\n",
    "    for dirpath, _, filenames in os.walk(dataset_root_path):\n",
    "        # 解析文件夹路径，以确定活动类别和具体活动\n",
    "        relative_path = os.path.relpath(dirpath, dataset_root_path)\n",
    "        path_parts = relative_path.split(os.sep)\n",
    "        # 确保只处理包含实际数据文件的特定层级文件夹\n",
    "        if len(path_parts) != 3: continue\n",
    "\n",
    "        # 遍历这些特定文件夹中的每一个文件\n",
    "        for filename in filenames:\n",
    "            # 确保只处理.txt文件\n",
    "            if not filename.endswith(\".txt\"): continue\n",
    "            \n",
    "            # 解析文件名，通过下划线分割以获取各个部分\n",
    "            fname_parts = filename.replace('.txt', '').split('_')\n",
    "            # 过滤掉不符合预期格式的文件名\n",
    "            if len(fname_parts) != 4: continue\n",
    "            \n",
    "            # 从文件名部分中提取所需信息\n",
    "            _, sensor_code, _, trial_no_str = fname_parts\n",
    "            # 将传感器代码转为小写以保持一致性\n",
    "            sensor_code = sensor_code.lower()\n",
    "            # 确保是已知的传感器类型 ('acc', 'gyro', 'ori')\n",
    "            if sensor_code not in SENSOR_CODES: continue\n",
    "\n",
    "            # 尝试从路径和文件名中提取并转换所有元数据\n",
    "            try:\n",
    "                # 从文件夹路径的第一部分提取受试者ID\n",
    "                subject_match = re.fullmatch(r'sub(\\d+)', path_parts[0], re.IGNORECASE)\n",
    "                if not subject_match: continue\n",
    "                subject_id = int(subject_match.group(1))\n",
    "                \n",
    "                # 从文件夹路径的第二和第三部分获取类别和活动代码\n",
    "                category = path_parts[1].upper()\n",
    "                activity_code = path_parts[2].upper()\n",
    "                # 将试验编号从字符串转换为整数\n",
    "                trial_no = int(trial_no_str)\n",
    "                # 构建完整的文件路径\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                \n",
    "                # 创建一个唯一的键来标识这次试验 (受试者, 活动, 试验编号)\n",
    "                trial_key = (subject_id, activity_code, trial_no)\n",
    "                # 在映射表中存储该传感器文件的路径\n",
    "                trial_sensor_files_map[trial_key][sensor_code] = filepath\n",
    "                # 如果是第一次遇到这个试验，则记录其元数据（类别和活动代码）\n",
    "                if trial_key not in trial_metadata_map:\n",
    "                    trial_metadata_map[trial_key] = {\"category\": category, \"activity_code\": activity_code}\n",
    "            except (AttributeError, ValueError):\n",
    "                # 如果在提取或转换过程中出现任何错误，则跳过该文件\n",
    "                continue\n",
    "\n",
    "    # 初始化两个列表，用于存放最终处理好的数据和对应的标签\n",
    "    processed_trials_data, labels = [], []\n",
    "    print(f\"\\nProcessing and combining {len(trial_sensor_files_map)} unique trials...\")\n",
    "    \n",
    "    # 遍历前面组织好的每一次活动试验（trial）\n",
    "    for trial_key, sensor_files in trial_sensor_files_map.items():\n",
    "        # 确保该次试验包含了 acc, gyro, ori 全部三种传感器文件，否则跳过\n",
    "        if not all(s_code in sensor_files for s_code in SENSOR_CODES): continue\n",
    "\n",
    "        # 使用字典推导式，为每种传感器加载并重采样数据\n",
    "        resampled_dfs = {s_code: load_and_resample_sensor_file(sensor_files[s_code], s_code) for s_code in SENSOR_CODES}\n",
    "        # 如果任何一个文件加载或处理失败（返回了None或空表），则跳过这次试验\n",
    "        if any(df is None or df.empty for df in resampled_dfs.values()): continue\n",
    "\n",
    "        try:\n",
    "            # --- 时间对齐关键步骤 ---\n",
    "            # 找到三个传感器数据中最晚的开始时间\n",
    "            common_start = max(df.index.min() for df in resampled_dfs.values())\n",
    "            # 找到三个传感器数据中最早的结束时间\n",
    "            common_end = min(df.index.max() for df in resampled_dfs.values())\n",
    "            # 如果没有重叠的时间窗口，则跳过\n",
    "            if common_start >= common_end: continue\n",
    "\n",
    "            # 将三个数据表都裁剪到共同的时间范围内\n",
    "            aligned_dfs = [resampled_dfs[s_code][common_start:common_end].reset_index(drop=True) for s_code in SENSOR_CODES]\n",
    "            # 确保对齐后的数据表长度一致且不为空，否则跳过\n",
    "            if not all(len(df) > 0 and len(df) == len(aligned_dfs[0]) for df in aligned_dfs): continue\n",
    "            \n",
    "            # --- 数据合并 ---\n",
    "            # 按列（axis=1）将三个对齐后的数据表拼接成一个宽表\n",
    "            combined_df = pd.concat(aligned_dfs, axis=1)\n",
    "            \n",
    "            # 再次检查并确保列名正确\n",
    "            if len(combined_df.columns) == len(ALL_FEATURE_COLUMNS):\n",
    "                 combined_df.columns = ALL_FEATURE_COLUMNS\n",
    "            else:\n",
    "                 continue # 如果列数不匹配则跳过\n",
    "\n",
    "            # 如果合并后的数据长度不足一个序列窗口（4秒），则跳过\n",
    "            if len(combined_df) < WINDOW_SIZE: continue\n",
    "            \n",
    "            # --- 数据和标签存储 ---\n",
    "            # 将处理好的数据（转换为Numpy数组）存入列表\n",
    "            processed_trials_data.append(combined_df.values)\n",
    "            # 根据元数据判断该试验是\"FALLS\"还是\"ADL\"，并存入标签（1代表跌倒，0代表非跌倒）\n",
    "            labels.append(1 if trial_metadata_map[trial_key][\"category\"] == \"FALLS\" else 0)\n",
    "            \n",
    "        except Exception:\n",
    "            # 捕获任何在对齐和合并过程中可能出现的意外错误，并跳过该试验\n",
    "            continue\n",
    "\n",
    "    print(f\"Successfully processed and combined sensor data for {len(processed_trials_data)} trials.\")\n",
    "    # 返回包含所有处理好的试验数据和标签的列表\n",
    "    return processed_trials_data, labels\n",
    "\n",
    "def create_sequences(data_list, label_list, seq_length, step):\n",
    "    \"\"\"使用滑动窗口从试验数据创建序列。\"\"\"\n",
    "    # 初始化用于存放最终序列和对应标签的列表\n",
    "    X, y = [], []\n",
    "    # 遍历每一次活动试验的数据\n",
    "    for i, trial_data in enumerate(data_list):\n",
    "        trial_label = label_list[i]\n",
    "        # 在单次试验数据上，按指定的步长（step）移动窗口\n",
    "        for j in range(0, len(trial_data) - seq_length + 1, step):\n",
    "            # 截取一个固定长度（seq_length）的片段作为序列\n",
    "            X.append(trial_data[j:(j + seq_length)])\n",
    "            # 为这个序列分配对应的标签\n",
    "            y.append(trial_label)\n",
    "            \n",
    "    if not X: return np.array([]), np.array([])\n",
    "    # 将列表转换为Numpy数组后返回\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SensorDataSequences, SensorLabelSequences = np.array([]), np.array([])\n",
    "\n",
    "if os.path.exists('SensorDataSequences.npy') and os.path.exists('SensorLabelSequences.npy'):\n",
    "    print(\"Found existing npy files. Loading...\")\n",
    "    SensorDataSequences = np.load('SensorDataSequences.npy')\n",
    "    print(f\"Loaded dataset shape: X={SensorDataSequences.shape}\")\n",
    "    SensorLabelSequences = np.load('SensorLabelSequences.npy')\n",
    "    print(f\"Loaded dataset shape: y={SensorLabelSequences.shape}\")\n",
    "else:\n",
    "    trial_arrays, trial_labels = load_data_from_structured_folders(DATASET_PATH)\n",
    "    SensorDataSequences, SensorLabelSequences = create_sequences(trial_arrays, trial_labels, WINDOW_SIZE, STEP)\n",
    "    print(f\"The shape of the final dataset is: X={SensorDataSequences.shape}, y={SensorLabelSequences.shape}\")\n",
    "    np.save('SensorDataSequences.npy', SensorDataSequences)\n",
    "    np.save('SensorLabelSequences.npy', SensorLabelSequences)\n",
    "    print(\"Saved processed dataset to npy files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f1a8a",
   "metadata": {},
   "source": [
    "## 自回归模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f3731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        # outputs shape: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden, cell shapes: (n_layers, batch_size, hidden_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # 我们将最后一个时间步的隐藏状态和细胞状态作为上下文向量\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch_size, 1, output_dim) - 初始输入是序列的最后一个点\n",
    "        # hidden, cell shapes: (n_layers, batch_size, hidden_dim)\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        # output shape: (batch_size, 1, hidden_dim)\n",
    "        prediction = self.fc(output)\n",
    "        # prediction shape: (batch_size, 1, output_dim)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        # source shape: (batch_size, input_len, input_dim)\n",
    "        # target shape: (batch_size, target_len, output_dim)\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_dim = self.decoder.output_dim\n",
    "\n",
    "        # 存储解码器输出的张量\n",
    "        outputs = torch.zeros(batch_size, target_len, target_dim).to(self.device)\n",
    "\n",
    "        # 编码器传递上下文向量\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # 解码器的第一个输入是源序列的最后一个时间点\n",
    "        # 这里为了简化，我们也可以使用一个全零的输入开始\n",
    "        decoder_input = torch.zeros(batch_size, 1, target_dim).to(self.device)\n",
    "\n",
    "        for t in range(target_len):\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs[:, t:t+1, :] = output\n",
    "\n",
    "            # 决定是否使用 \"Teacher Forcing\"\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            # 如果使用，下一个输入是真实的标签；否则，使用模型自己的预测\n",
    "            decoder_input = target[:, t:t+1, :] if teacher_force else output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32d067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 基本设置 ---\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 为了结果可复现，设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_len, target_len):\n",
    "        self.data = data\n",
    "        self.input_len = input_len\n",
    "        self.target_len = target_len\n",
    "        # 总序列长度必须等于 input_len + target_len\n",
    "        assert data.shape[1] == input_len + target_len, \"Data sequence length does not match input_len + target_len\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        x = sequence[:self.input_len]\n",
    "        y = sequence[self.input_len:]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# --- 定义序列长度 ---\n",
    "# 使用前150个点来预测后50个点\n",
    "INPUT_SEQ_LEN = 150\n",
    "TARGET_SEQ_LEN = 50\n",
    "TOTAL_SEQ_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001763b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully standardized.\n",
      "Original train data mean (first feature): -0.25784138838159726\n",
      "Scaled train data mean (first feature): -4.016325967507197e-15\n",
      "Scaled train data std (first feature): 0.9999999999999226\n",
      "\n",
      "Starting model training...\n",
      "Epoch: 01 | Time: 0m 9s\n",
      "\tTrain Loss: 0.7788\n",
      "\t Val. Loss: 0.6769\n",
      "\tBest model saved with validation loss: 0.6769\n",
      "Epoch: 02 | Time: 0m 9s\n",
      "\tTrain Loss: 0.4906\n",
      "\t Val. Loss: 0.5967\n",
      "\tBest model saved with validation loss: 0.5967\n",
      "Epoch: 03 | Time: 0m 9s\n",
      "\tTrain Loss: 0.3640\n",
      "\t Val. Loss: 0.5644\n",
      "\tBest model saved with validation loss: 0.5644\n",
      "Epoch: 04 | Time: 0m 9s\n",
      "\tTrain Loss: 0.3092\n",
      "\t Val. Loss: 0.5463\n",
      "\tBest model saved with validation loss: 0.5463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# 5. 开始训练\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, targets) \u001b[38;5;66;03m# Teacher forcing is on by default\u001b[39;00m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\ghost\\anaconda3\\envs\\falldetect_torch\\lib\\site-packages\\torch\\_tensor.py:592\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward\u001b[39m(\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    594\u001b[0m ):\n\u001b[0;32m    595\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`. Defaults to ``None``.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs):\n",
    "    \"\"\"\n",
    "    模型训练主函数\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- 训练模式 ---\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, targets) # Teacher forcing is on by default\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # --- 评估模式 ---\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                # 在评估时关闭 teacher forcing\n",
    "                outputs = model(inputs, targets, teacher_forcing_ratio=0) \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # --- 打印日志 ---\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "        print(f'\\tTrain Loss: {avg_train_loss:.4f}')\n",
    "        print(f'\\t Val. Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # 保存表现最好的模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'autoregression_feature_extractor_model.pt')\n",
    "            print(f'\\tBest model saved with validation loss: {best_val_loss:.4f}')\n",
    "\n",
    "# --- 主执行流程 ---\n",
    "\n",
    "# 1. 设置超参数\n",
    "INPUT_DIM = 11      # 每个时间点的特征数\n",
    "OUTPUT_DIM = 11     # 输出维度和输入维度相同\n",
    "HIDDEN_DIM = 64     # 隐藏层维度，这个值决定了特征向量的维度，可以调整\n",
    "N_LAYERS = 2        # LSTM层数\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# 1. 划分数据集\n",
    "train_data, val_data = train_test_split(SensorDataSequences, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. 创建并拟合Scaler\n",
    "# 需要将数据 reshape 成 2D 来进行拟合\n",
    "# (num_samples * seq_len, num_features)\n",
    "num_train_samples, seq_len, num_features = train_data.shape\n",
    "num_val_samples = val_data.shape[0]\n",
    "\n",
    "# 只用训练数据来拟合Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data.reshape(-1, num_features))\n",
    "\n",
    "# 3. 标准化训练集和验证集\n",
    "train_data_scaled = scaler.transform(train_data.reshape(-1, num_features)).reshape(num_train_samples, seq_len, num_features)\n",
    "val_data_scaled = scaler.transform(val_data.reshape(-1, num_features)).reshape(num_val_samples, seq_len, num_features)\n",
    "\n",
    "print(\"Data successfully standardized.\")\n",
    "print(\"Original train data mean (first feature):\", np.mean(train_data[:,:,0]))\n",
    "print(\"Scaled train data mean (first feature):\", np.mean(train_data_scaled[:,:,0])) # 应该接近 0\n",
    "print(\"Scaled train data std (first feature):\", np.std(train_data_scaled[:,:,0]))\n",
    "      \n",
    "    \n",
    "# 3. 创建Dataset和DataLoader\n",
    "train_dataset = TimeSeriesDataset(train_data_scaled, INPUT_SEQ_LEN, TARGET_SEQ_LEN)\n",
    "val_dataset = TimeSeriesDataset(val_data_scaled, INPUT_SEQ_LEN, TARGET_SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 4. 实例化模型、优化器和损失函数\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss() # 均方误差损失，适合回归预测任务\n",
    "\n",
    "# 5. 开始训练\n",
    "print(\"\\nStarting model training...\")\n",
    "train_model(model, train_loader, val_loader, optimizer, criterion, EPOCHS)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18664139",
   "metadata": {},
   "source": [
    "## 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fbcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data processing and chunking...\n",
      "Saving final chunk 1 with 186 sequences...\n",
      "------------------------------\n",
      "Processing complete.\n",
      "Total sequences generated: 186\n",
      "Total chunks saved: 1\n",
      "Data saved in directory: './processed_data'\n"
     ]
    }
   ],
   "source": [
    "# -- 窗口化参数 --\n",
    "# 每个序列的长度 (即时间步数)\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH_FEATURE = 200\n",
    "# 滑动窗口的步长。\n",
    "# 如果 STEP = 200，窗口之间没有重叠。\n",
    "# 如果 STEP < 200，窗口之间有重叠，可以起到数据增强的作用。\n",
    "# 如果 STEP = 1，则会生成最多的数据。这里我们设为50，表示75%的重叠。\n",
    "STEP_FEATURE = 50\n",
    "\n",
    "# -- 分块保存参数 --\n",
    "# 每个数据块中包含多少个序列。这可以避免单个文件过大，方便传输和加载。\n",
    "NUM_SEQUENCES_PER_CHUNK = 1000\n",
    "# 处理后的数据保存目录\n",
    "OUTPUT_DIR = \"./autoregression_processed_data\"\n",
    "# 保存文件的前缀\n",
    "FILE_PREFIX = \"fall_detection_chunk\"\n",
    "\n",
    "# --- 3. 核心处理与保存函数 ---\n",
    "def process_and_save_data(raw_data, raw_labels, seq_len, step, chunk_size, out_dir, prefix):\n",
    "    \"\"\"\n",
    "    使用滑动窗口处理数据，并分块保存。\n",
    "\n",
    "    参数:\n",
    "        - raw_data: 原始连续传感器数据 (N, num_features)\n",
    "        - raw_labels: 原始连续标签 (N,)\n",
    "        - seq_len: 每个序列的长度\n",
    "        - step: 窗口滑动的步长\n",
    "        - chunk_size: 每个分块文件包含的序列数\n",
    "        - out_dir: 输出目录\n",
    "        - prefix: 文件前缀\n",
    "    \"\"\"\n",
    "    print(\"Starting data processing and chunking...\")\n",
    "    \n",
    "    # 如果输出目录已存在，先清空\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    chunk_count = 0\n",
    "\n",
    "    total_len = len(raw_data)\n",
    "    num_sequences_generated = 0\n",
    "\n",
    "    for i in range(0, total_len - seq_len + 1, step):\n",
    "        # 提取一个窗口的数据\n",
    "        window_data = raw_data[i : i + seq_len]\n",
    "        window_labels = raw_labels[i : i + seq_len]\n",
    "\n",
    "        sequences.append(window_data)\n",
    "        \n",
    "        # 确定该窗口的标签：只要窗口内包含任何一个跌倒标签(1)，整个序列就标记为跌倒。\n",
    "        # 这是处理序列分类任务时的常用策略。\n",
    "        label = 1 if np.any(window_labels == 1) else 0\n",
    "        labels.append(label)\n",
    "        \n",
    "        num_sequences_generated += 1\n",
    "\n",
    "        # 当累积的序列达到一个分块大小时，就保存\n",
    "        if len(sequences) == chunk_size:\n",
    "            chunk_count += 1\n",
    "            print(f\"Saving chunk {chunk_count} with {len(sequences)} sequences...\")\n",
    "            \n",
    "            # 转换为Numpy数组\n",
    "            sequences_np = np.array(sequences, dtype=np.float32)\n",
    "            labels_np = np.array(labels, dtype=np.int64)\n",
    "\n",
    "            # 定义文件名\n",
    "            data_filename = os.path.join(out_dir, f\"{prefix}_{chunk_count}_data.npy\")\n",
    "            labels_filename = os.path.join(out_dir, f\"{prefix}_{chunk_count}_labels.npy\")\n",
    "\n",
    "            # 保存文件\n",
    "            np.save(data_filename, sequences_np)\n",
    "            np.save(labels_filename, labels_np)\n",
    "\n",
    "            # 清空列表以准备下一个分块\n",
    "            sequences = []\n",
    "            labels = []\n",
    "\n",
    "    # 保存最后一个不足一个chunk大小的剩余数据\n",
    "    if sequences:\n",
    "        chunk_count += 1\n",
    "        print(f\"Saving final chunk {chunk_count} with {len(sequences)} sequences...\")\n",
    "        sequences_np = np.array(sequences, dtype=np.float32)\n",
    "        labels_np = np.array(labels, dtype=np.int64)\n",
    "        \n",
    "        data_filename = os.path.join(out_dir, f\"{prefix}_{chunk_count}_data.npy\")\n",
    "        labels_filename = os.path.join(out_dir, f\"{prefix}_{chunk_count}_labels.npy\")\n",
    "\n",
    "        np.save(data_filename, sequences_np)\n",
    "        np.save(labels_filename, labels_np)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Processing complete.\")\n",
    "    print(f\"Total sequences generated: {num_sequences_generated}\")\n",
    "    print(f\"Total chunks saved: {chunk_count}\")\n",
    "    print(f\"Data saved in directory: '{out_dir}'\")\n",
    "\n",
    "\n",
    "# --- 4. 主执行函数 ---\n",
    "# 步骤 A: 加载您的原始数据\n",
    "continuous_data, continuous_labels = SensorDataSequences, SensorLabelSequences\n",
    "\n",
    "# 步骤 B: 调用处理函数\n",
    "process_and_save_data(\n",
    "    raw_data=continuous_data,\n",
    "    raw_labels=continuous_labels,\n",
    "    seq_len=SEQUENCE_LENGTH_FEATURE,\n",
    "    step=STEP_FEATURE,\n",
    "    chunk_size=NUM_SEQUENCES_PER_CHUNK,\n",
    "    out_dir=OUTPUT_DIR,\n",
    "    prefix=FILE_PREFIX\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

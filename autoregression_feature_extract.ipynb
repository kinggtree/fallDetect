{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 模型参数\n",
    "INPUT_DIM = 11\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f948b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：这里的模型定义需要和训练时的代码保持完全一致\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "        \n",
    "# --- 核心：特征提取器类 ---\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_path, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        \"\"\"\n",
    "        初始化特征提取器。\n",
    "\n",
    "        参数:\n",
    "            model_path (str): 训练好的完整Seq2Seq模型权重文件路径 (例如 'feature_extractor_model.pt')\n",
    "            input_dim (int): 输入特征维度 (11)\n",
    "            hidden_dim (int): LSTM隐藏层维度，也即输出特征的维度 (例如 64)\n",
    "            n_layers (int): LSTM层数 (例如 2)\n",
    "            dropout (float): Dropout比例，加载时应与训练时一致\n",
    "        \"\"\"\n",
    "        # 检查设备\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"FeatureExtractor is using device: {self.device}\")\n",
    "\n",
    "        # 1. 实例化Encoder\n",
    "        # 我们只需要加载Encoder部分即可\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, n_layers, dropout).to(self.device)\n",
    "\n",
    "        # 2. 加载训练好的权重\n",
    "        # 注意：我们加载的是包含Encoder和Decoder的完整模型权重，\n",
    "        # 但PyTorch会自动匹配名称，只加载Encoder对应的权重。\n",
    "        # 为了更干净，可以只保存和加载encoder的state_dict，但这样也能工作。\n",
    "        \n",
    "        # 创建一个临时的完整模型来加载字典\n",
    "        # (这是一个小技巧，以避免处理state_dict中不匹配的键)\n",
    "        class DummyDecoder(nn.Module):\n",
    "            def __init__(self): super().__init__(); self.dummy = nn.Linear(1,1)\n",
    "        \n",
    "        class DummySeq2Seq(nn.Module):\n",
    "            def __init__(self, encoder):\n",
    "                super().__init__()\n",
    "                self.encoder = encoder\n",
    "                self.decoder = DummyDecoder() # 只是占位\n",
    "        \n",
    "        temp_full_model = DummySeq2Seq(self.encoder)\n",
    "        temp_full_model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        \n",
    "        print(f\"Successfully loaded encoder weights from {model_path}\")\n",
    "\n",
    "        # 3. 设置为评估模式\n",
    "        # 这是非常重要的一步！它会关闭Dropout等训练特有的层。\n",
    "        self.encoder.eval()\n",
    "\n",
    "    def extract_feature(self, sequence_data):\n",
    "        \"\"\"\n",
    "        从一个4秒(200个点)的序列中提取特征向量。\n",
    "\n",
    "        参数:\n",
    "            sequence_data (np.ndarray): 输入的传感器数据，形状必须为 (200, 11)\n",
    "\n",
    "        返回:\n",
    "            np.ndarray: 提取出的特征向量，形状为 (hidden_dim,)\n",
    "        \"\"\"\n",
    "        # --- 输入验证 ---\n",
    "        if not isinstance(sequence_data, np.ndarray) or sequence_data.shape != (200, 11):\n",
    "            raise ValueError(\"Input data must be a numpy array of shape (200, 11)\")\n",
    "\n",
    "        # --- 特征提取核心逻辑 ---\n",
    "        with torch.no_grad(): # 关闭梯度计算，加速推理\n",
    "            # 1. 将Numpy数组转换为PyTorch张量\n",
    "            input_tensor = torch.tensor(sequence_data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # 2. 增加Batch维度\n",
    "            # 模型的LSTM层期望的输入是 (batch_size, seq_len, input_dim)\n",
    "            # 所以 (200, 11) 需要变成 (1, 200, 11)\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "            # 3. 通过Encoder进行前向传播\n",
    "            hidden_state, _ = self.encoder(input_tensor)\n",
    "            # hidden_state 的形状是 (n_layers, batch_size, hidden_dim)\n",
    "\n",
    "            # 4. 提取我们需要的特征向量\n",
    "            # 通常我们使用最后一层的隐藏状态作为特征\n",
    "            feature_vector_tensor = hidden_state[-1, :, :] # 取最后一层, shape: (1, hidden_dim)\n",
    "\n",
    "            # 5. 去掉Batch维度，并转换回Numpy数组\n",
    "            feature_vector_tensor = feature_vector_tensor.squeeze(0) # Shape: (hidden_dim)\n",
    "            feature_vector_np = feature_vector_tensor.cpu().numpy()\n",
    "\n",
    "            return feature_vector_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea532b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 使用流程 ---\n",
    "print(\"\\n--- Initializing Feature Extractor ---\")\n",
    "# 2. 定义模型参数 (必须和训练时完全一致)\n",
    "MODEL_PARAMS = {\n",
    "    'model_path': 'feature_extractor_model.pt',\n",
    "    'input_dim': INPUT_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# 3. 实例化提取器\n",
    "# 在您的边缘节点程序启动时，只需要执行一次这个实例化操作\n",
    "extractor = FeatureExtractor(**MODEL_PARAMS)\n",
    "\n",
    "print(\"\\n--- Performing Feature Extraction ---\")\n",
    "# 4. 模拟从传感器获得的一个4秒（200个点）的数据\n",
    "# 在实际应用中，这将是您实时收集的数据\n",
    "new_sensor_data = np.random.rand(200, 11).astype(np.float32)\n",
    "print(f\"Input data shape: {new_sensor_data.shape}\")\n",
    "\n",
    "# 5. 调用方法提取特征\n",
    "feature = extractor.extract_feature(new_sensor_data)\n",
    "\n",
    "# 6. 查看结果\n",
    "print(f\"Successfully extracted feature vector.\")\n",
    "print(f\"Feature vector shape: {feature.shape}\")\n",
    "print(\"This feature vector can now be sent to the main server.\")\n",
    "print(\"Feature vector sample:\", feature[:5]) # 查看具体数值"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

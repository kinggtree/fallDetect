{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036a158e",
   "metadata": {},
   "source": [
    "# 注意\n",
    "请确保SensorDataSequences.npy, SensorLabelSequences.npy存在，且维度为(x,200,11)\n",
    "生成部分可以参见 fidelity_model_train_new.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23de2bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 模型参数\n",
    "INPUT_DIM = 11\n",
    "HIDDEN_DIM = 64\n",
    "N_LAYERS = 2\n",
    "MODEL_PATH = 'autoregression_feature_extractor_model.pt'\n",
    "DROP_OUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f948b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：这里的模型定义需要和训练时的代码保持完全一致\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "        \n",
    "# --- 核心：特征提取器类 ---\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, model_path, input_dim, hidden_dim, n_layers, dropout=0.0):\n",
    "        \"\"\"\n",
    "        初始化特征提取器。\n",
    "        \"\"\"\n",
    "        # 检查设备\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"FeatureExtractor is using device: {self.device}\")\n",
    "\n",
    "        # 1. 实例化我们需要的Encoder模型\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, n_layers, dropout).to(self.device)\n",
    "\n",
    "        # 2. 加载训练好的完整Seq2Seq模型的权重字典\n",
    "        full_state_dict = torch.load(model_path, map_location=self.device)\n",
    "\n",
    "        # 3. 创建一个新的字典，只包含Encoder的权重\n",
    "        #    并移除键名前缀 \"encoder.\"\n",
    "        encoder_state_dict = {}\n",
    "        for key, value in full_state_dict.items():\n",
    "            if key.startswith('encoder.'):\n",
    "                # 将 'encoder.lstm.weight_ih_l0' 变为 'lstm.weight_ih_l0'\n",
    "                new_key = key[len('encoder.'):] \n",
    "                encoder_state_dict[new_key] = value\n",
    "        \n",
    "        # 4. 将筛选后的权重加载到Encoder模型中\n",
    "        self.encoder.load_state_dict(encoder_state_dict)\n",
    "        \n",
    "        print(f\"Successfully loaded encoder weights from {model_path}\")\n",
    "\n",
    "        # 5. 设置为评估模式\n",
    "        self.encoder.eval()\n",
    "\n",
    "    def extract_feature(self, sequence_data):\n",
    "        \"\"\"\n",
    "        从一个4秒(200个点)的序列中提取特征向量。\n",
    "\n",
    "        参数:\n",
    "            sequence_data (np.ndarray): 输入的传感器数据，形状必须为 (200, 11)\n",
    "\n",
    "        返回:\n",
    "            np.ndarray: 提取出的特征向量，形状为 (hidden_dim,)\n",
    "        \"\"\"\n",
    "        # --- 输入验证 ---\n",
    "        if not isinstance(sequence_data, np.ndarray) or sequence_data.shape != (200, 11):\n",
    "            raise ValueError(\"Input data must be a numpy array of shape (200, 11)\")\n",
    "\n",
    "        # --- 特征提取核心逻辑 ---\n",
    "        with torch.no_grad(): # 关闭梯度计算，加速推理\n",
    "            # 1. 将Numpy数组转换为PyTorch张量\n",
    "            input_tensor = torch.tensor(sequence_data, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # 2. 增加Batch维度\n",
    "            # 模型的LSTM层期望的输入是 (batch_size, seq_len, input_dim)\n",
    "            # 所以 (200, 11) 需要变成 (1, 200, 11)\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "            # 3. 通过Encoder进行前向传播\n",
    "            hidden_state, _ = self.encoder(input_tensor)\n",
    "            # hidden_state 的形状是 (n_layers, batch_size, hidden_dim)\n",
    "\n",
    "            # 4. 提取我们需要的特征向量\n",
    "            # 通常我们使用最后一层的隐藏状态作为特征\n",
    "            feature_vector_tensor = hidden_state[-1, :, :] # 取最后一层, shape: (1, hidden_dim)\n",
    "\n",
    "            # 5. 去掉Batch维度，并转换回Numpy数组\n",
    "            feature_vector_tensor = feature_vector_tensor.squeeze(0) # Shape: (hidden_dim)\n",
    "            feature_vector_np = feature_vector_tensor.cpu().numpy()\n",
    "\n",
    "            return feature_vector_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea532b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing Feature Extractor ---\n",
      "FeatureExtractor is using device: cuda\n",
      "Successfully loaded encoder weights from autoregression_feature_extractor_model.pt\n",
      "\n",
      "--- Loading data and extracting features ---\n",
      "Loaded data with shape: (9491, 200, 11)\n",
      "Loaded labels with shape: (9491,)\n",
      "Processed 500/9491 sequences...\n",
      "Processed 1000/9491 sequences...\n",
      "Processed 1500/9491 sequences...\n",
      "Processed 2000/9491 sequences...\n",
      "Processed 2500/9491 sequences...\n",
      "Processed 3000/9491 sequences...\n",
      "Processed 3500/9491 sequences...\n",
      "Processed 4000/9491 sequences...\n",
      "Processed 4500/9491 sequences...\n",
      "Processed 5000/9491 sequences...\n",
      "Processed 5500/9491 sequences...\n",
      "Processed 6000/9491 sequences...\n",
      "Processed 6500/9491 sequences...\n",
      "Processed 7000/9491 sequences...\n",
      "Processed 7500/9491 sequences...\n",
      "Processed 8000/9491 sequences...\n",
      "Processed 8500/9491 sequences...\n",
      "Processed 9000/9491 sequences...\n",
      "Processed 9491/9491 sequences...\n",
      "\n",
      "Feature extraction complete. Final features array shape: (9491, 64)\n",
      "\n",
      "--- Saving features and labels into batches of size 256 ---\n",
      "Saved batch 1/38: features (256, 64), labels (256,)\n",
      "Saved batch 2/38: features (256, 64), labels (256,)\n",
      "Saved batch 3/38: features (256, 64), labels (256,)\n",
      "Saved batch 4/38: features (256, 64), labels (256,)\n",
      "Saved batch 5/38: features (256, 64), labels (256,)\n",
      "Saved batch 6/38: features (256, 64), labels (256,)\n",
      "Saved batch 7/38: features (256, 64), labels (256,)\n",
      "Saved batch 8/38: features (256, 64), labels (256,)\n",
      "Saved batch 9/38: features (256, 64), labels (256,)\n",
      "Saved batch 10/38: features (256, 64), labels (256,)\n",
      "Saved batch 11/38: features (256, 64), labels (256,)\n",
      "Saved batch 12/38: features (256, 64), labels (256,)\n",
      "Saved batch 13/38: features (256, 64), labels (256,)\n",
      "Saved batch 14/38: features (256, 64), labels (256,)\n",
      "Saved batch 15/38: features (256, 64), labels (256,)\n",
      "Saved batch 16/38: features (256, 64), labels (256,)\n",
      "Saved batch 17/38: features (256, 64), labels (256,)\n",
      "Saved batch 18/38: features (256, 64), labels (256,)\n",
      "Saved batch 19/38: features (256, 64), labels (256,)\n",
      "Saved batch 20/38: features (256, 64), labels (256,)\n",
      "Saved batch 21/38: features (256, 64), labels (256,)\n",
      "Saved batch 22/38: features (256, 64), labels (256,)\n",
      "Saved batch 23/38: features (256, 64), labels (256,)\n",
      "Saved batch 24/38: features (256, 64), labels (256,)\n",
      "Saved batch 25/38: features (256, 64), labels (256,)\n",
      "Saved batch 26/38: features (256, 64), labels (256,)\n",
      "Saved batch 27/38: features (256, 64), labels (256,)\n",
      "Saved batch 28/38: features (256, 64), labels (256,)\n",
      "Saved batch 29/38: features (256, 64), labels (256,)\n",
      "Saved batch 30/38: features (256, 64), labels (256,)\n",
      "Saved batch 31/38: features (256, 64), labels (256,)\n",
      "Saved batch 32/38: features (256, 64), labels (256,)\n",
      "Saved batch 33/38: features (256, 64), labels (256,)\n",
      "Saved batch 34/38: features (256, 64), labels (256,)\n",
      "Saved batch 35/38: features (256, 64), labels (256,)\n",
      "Saved batch 36/38: features (256, 64), labels (256,)\n",
      "Saved batch 37/38: features (256, 64), labels (256,)\n",
      "Saved batch 38/38: features (19, 64), labels (19,)\n"
     ]
    }
   ],
   "source": [
    "# 你的完整数据文件路径\n",
    "FULL_DATA_PATH = 'SensorDataSequences.npy'\n",
    "FULL_LABEL_PATH = 'SensorLabelSequences.npy'\n",
    "\n",
    "# 在这里自定义批次大小\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# 输出文件的保存目录\n",
    "OUTPUT_DIR = 'extracted_features'\n",
    "\n",
    "# 创建输出目录（如果不存在）\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "# --- 2. 初始化特征提取器 ---\n",
    "print(\"\\n--- Initializing Feature Extractor ---\")\n",
    "extractor = FeatureExtractor(\n",
    "    model_path=MODEL_PATH,\n",
    "    input_dim=INPUT_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROP_OUT\n",
    ")\n",
    "\n",
    "# --- 3. 加载数据并逐个提取特征 ---\n",
    "print(\"\\n--- Loading data and extracting features ---\")\n",
    "all_sequences = np.load(FULL_DATA_PATH)\n",
    "all_labels = np.load(FULL_LABEL_PATH) # <-- 新增：加载标签\n",
    "\n",
    "# 验证一下数据和标签的数量是否一致\n",
    "num_sequences = all_sequences.shape[0]\n",
    "assert num_sequences == all_labels.shape[0], \\\n",
    "    f\"数据和标签的数量不匹配! 数据有 {num_sequences} 个, 标签有 {all_labels.shape[0]} 个。\"\n",
    "\n",
    "print(f\"Loaded data with shape: {all_sequences.shape}\")\n",
    "print(f\"Loaded labels with shape: {all_labels.shape}\") # <-- 新增：打印标签形状\n",
    "\n",
    "all_features = []\n",
    "for i in range(num_sequences):\n",
    "    sequence = all_sequences[i] # 取出第 i 个序列\n",
    "    feature = extractor.extract_feature(sequence) # 提取特征\n",
    "    all_features.append(feature) # 添加到列表中\n",
    "    \n",
    "    # 打印进度，方便观察\n",
    "    if (i + 1) % 500 == 0 or (i + 1) == num_sequences:\n",
    "        print(f\"Processed {i + 1}/{num_sequences} sequences...\")\n",
    "\n",
    "# 将特征列表转换为一个大的Numpy数组\n",
    "all_features_np = np.array(all_features)\n",
    "print(f\"\\nFeature extraction complete. Final features array shape: {all_features_np.shape}\")\n",
    "\n",
    "# --- 4. 将所有特征和标签分批保存到文件 ---\n",
    "print(f\"\\n--- Saving features and labels into batches of size {BATCH_SIZE} ---\")\n",
    "num_batches = (num_sequences + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * BATCH_SIZE\n",
    "    end_idx = min((i + 1) * BATCH_SIZE, num_sequences)\n",
    "    \n",
    "    # --- 切片特征和标签 ---\n",
    "    batch_features = all_features_np[start_idx:end_idx]\n",
    "    batch_labels = all_labels[start_idx:end_idx] # <-- 新增：切片标签\n",
    "    \n",
    "    # --- 保存特征批次 ---\n",
    "    output_feature_filename = os.path.join(OUTPUT_DIR, f'features_batch_{i}.npy')\n",
    "    np.save(output_feature_filename, batch_features)\n",
    "    \n",
    "    # --- 保存标签批次 ---\n",
    "    output_label_filename = os.path.join(OUTPUT_DIR, f'labels_batch_{i}.npy') # <-- 新增\n",
    "    np.save(output_label_filename, batch_labels) # <-- 新增\n",
    "    \n",
    "    # 更新打印信息\n",
    "    print(f\"Saved batch {i+1}/{num_batches}: features {batch_features.shape}, labels {batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51d3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 38 个特征文件和 38 个标签文件。\n",
      "\n",
      "合并完成！\n",
      "  - 特征文件已保存: all_features.npy, 形状: (9491, 64)\n",
      "  - 标签文件已保存: all_labels.npy, 形状: (9491,)\n"
     ]
    }
   ],
   "source": [
    "# 存放分批文件的文件夹\n",
    "BATCH_DIR = 'extracted_features' \n",
    "# 合并后的特征文件名\n",
    "OUTPUT_FEATURES_FILE = 'all_features.npy'\n",
    "# 合并后的标签文件名\n",
    "OUTPUT_LABELS_FILE = 'all_labels.npy'\n",
    "\n",
    "\n",
    "# 查找所有分批的特征文件并排序，确保顺序正确\n",
    "feature_files = sorted(glob.glob(os.path.join(BATCH_DIR, 'features_batch_*.npy')))\n",
    "label_files = sorted(glob.glob(os.path.join(BATCH_DIR, 'labels_batch_*.npy')))\n",
    "\n",
    "if not feature_files or not label_files:\n",
    "    print(f\"错误：在文件夹 '{BATCH_DIR}' 中找不到任何分批文件。\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"找到 {len(feature_files)} 个特征文件和 {len(label_files)} 个标签文件。\")\n",
    "\n",
    "# 加载并合并所有特征\n",
    "all_features = [np.load(f) for f in feature_files]\n",
    "consolidated_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "# 加载并合并所有标签\n",
    "all_labels = [np.load(f) for f in label_files]\n",
    "consolidated_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# 保存合并后的文件\n",
    "np.save(OUTPUT_FEATURES_FILE, consolidated_features)\n",
    "np.save(OUTPUT_LABELS_FILE, consolidated_labels)\n",
    "\n",
    "print(\"\\n合并完成！\")\n",
    "print(f\"  - 特征文件已保存: {OUTPUT_FEATURES_FILE}, 形状: {consolidated_features.shape}\")\n",
    "print(f\"  - 标签文件已保存: {OUTPUT_LABELS_FILE}, 形状: {consolidated_labels.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falldetect_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
